{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import math\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4 # make larger to let test go fast. f=1 is target size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(x,y):\n",
    "    return ((x.reshape(-1).double() * y.reshape(-1).double()).sum() / x.reshape(-1).double().norm() / y.reshape(-1).double().norm()).float()\n",
    "\n",
    "@torch._dynamo.disable\n",
    "def baseline_torch(x, y, A):\n",
    "    V = A.shape[0]\n",
    "    return F.cross_entropy(F.linear(x, A).view(-1, V).float(), y.view(-1))\n",
    "\n",
    "@torch.compile # need to define this twice, otherwise there's weird shadowing happening in the notebook\n",
    "def compiled_baseline(x, y, A):\n",
    "    V = A.shape[0]\n",
    "    return F.cross_entropy(F.linear(x, A).view(-1, V).float(), y.view(-1))\n",
    "\n",
    "maxauto_baseline = torch.compile(baseline_torch, fullgraph=True, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, H, V = (4096 * 4) // f, 4096 // f, 131072 // f\n",
    "# N, H, V =256, 1024 * 4, 32768\n",
    "\n",
    "compute_dtype = torch.float16\n",
    "\n",
    "y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "A = torch.randn(V, H, requires_grad=True, device=device, dtype=compute_dtype)\n",
    "At = A.clone().detach().T.contiguous()\n",
    "At.requires_grad_()\n",
    "\n",
    "# x = torch.randn(B * S, H, requires_grad=True, device=device, dtype=torch.float32) # B S H\n",
    "# x = A[y].clone().detach()\n",
    "x = 0.1 * A[y].clone().detach() + torch.randn(N, H, device=device, dtype=compute_dtype)\n",
    "x.requires_grad_()\n",
    "\n",
    "loss = baseline_torch(x.float(), y, A.float())\n",
    "loss.backward()\n",
    "\n",
    "reference_A_grad = A.grad.float().clone()\n",
    "reference_x_grad = x.grad.float().clone()\n",
    "reference_loss = loss.detach().float().clone()\n",
    "\n",
    "z_ref = F.linear(x, A).view(-1, V).float().detach()\n",
    "m_ref = z_ref.max(dim=1)[0]\n",
    "s_ref = (z_ref - m_ref[:, None]).exp().sum(dim=1)\n",
    "\n",
    "print(reference_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bench(fn, reference_loss, reference_x_grad, reference_A_grad):\n",
    "    x.grad, A.grad, At.grad = None, None, None\n",
    "    loss_triton = fn() # warmup\n",
    "    torch.cuda.synchronize()\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    start_event.record()\n",
    "    loss_triton = fn()\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    estimate_ms_fwd = start_event.elapsed_time(end_event)\n",
    "    print(f\"fwd : {estimate_ms_fwd}ms\")\n",
    "    print(f\"fwd error: {torch.dist(loss, reference_loss).item()}\")\n",
    "    loss_triton = fn()\n",
    "    loss_triton.backward() # warmup\n",
    "    x.grad, A.grad, At.grad = None, None, None\n",
    "    loss_triton = fn()\n",
    "    torch.cuda.synchronize()\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    start_event.record()\n",
    "    loss_triton.backward()\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    estimate_ms_bwd = start_event.elapsed_time(end_event)\n",
    "    print(f\"bwd : {estimate_ms_bwd}ms\")\n",
    "    if At.grad is not None:\n",
    "        A_error = torch.dist(reference_A_grad.T, At.grad).item()\n",
    "    else:\n",
    "        A_error = torch.dist(reference_A_grad, A.grad).item()\n",
    "    print(f\"bwd error: {torch.dist(reference_x_grad, x.grad).item()}, {A_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _inner_function(x_block, y_block, A, num_blocks):\n",
    "    return F.cross_entropy(F.linear(x_block, A), y_block) / num_blocks\n",
    "\n",
    "# @torch.compile(dynamic=False)\n",
    "def torch_compiled_checkpoint(x, y, A, default_chunk_size = 512):\n",
    "    loss = 0.\n",
    "    N = x.view(-1, H).shape[0]\n",
    "    chunk_size = min(default_chunk_size, N)\n",
    "    if chunk_size % N != 0:\n",
    "        chunk_size = math.gcd(N, default_chunk_size)\n",
    "    x_blocks = x.view(-1, H).split(chunk_size)\n",
    "    y_blocks = y.view(-1).split(chunk_size)\n",
    "\n",
    "\n",
    "    for x_block, y_block in zip(x_blocks, y_blocks):\n",
    "        loss += checkpoint(_inner_function, x_block, y_block, A, num_blocks=len(y_blocks), use_reentrant=False)\n",
    "    return loss\n",
    "torch_compiled_checkpoint(x, y, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_bench(lambda: compiled_baseline(x,y,A), reference_loss, reference_x_grad, reference_A_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_bench(lambda: torch_compiled_checkpoint(x,y,A), reference_loss, reference_x_grad, reference_A_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from malek_xent import linear_cross_entropy as efficient_xent, FusedProjectionPlusCrossEntropyLoss\n",
    "\n",
    "\n",
    "# op = FusedProjectionPlusCrossEntropyLoss(H, V, 16).to(device)\n",
    "# op(x, y).backward()\n",
    "\n",
    "\n",
    "\n",
    "simple_bench(lambda: efficient_xent(x,y,A), reference_loss, reference_x_grad, reference_A_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from double_recomp2 import linear_cross_entropy as linear_cross_entropy_double_recomp\n",
    "\n",
    "\n",
    "simple_bench(lambda: linear_cross_entropy_double_recomp(x,y,At), reference_loss, reference_x_grad, reference_A_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, A, At"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try eventually:\n",
    "# from litgpt.ops import linear_cross_entropy_checkerboard\n",
    "# from litgpt.ops import linear_cross_entropy_nolock\n",
    "# from litgpt.ops import linear_cross_entropy\n",
    "# from litgpt.ops import linear_cross_entropy_double_recomp\n",
    "\n",
    "from litgpt.ops import linear_cross_entropy_nolock\n",
    "from highmem import linear_cross_entropy as linear_cross_entropy_highmem\n",
    "from double_recomp2 import linear_cross_entropy as linear_cross_entropy_double_recomp\n",
    "from double_recomp3 import linear_cross_entropy as linear_cross_entropy_parallel_recomp\n",
    "from manyway_recomp import linear_cross_entropy as linear_cross_entropy_manyway\n",
    "from malek_xent import linear_cross_entropy as efficient_xent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking FWD + BWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_dict = {'H':range(10, 14, 1),'V': range(10, 18, 1),'N': range(8, 15, 1)}\n",
    "\n",
    "configs = []\n",
    "for mode in [\"fwd\", \"bwd\", \"fwd-bwd\"]: # , \n",
    "    for variable in ['H', 'N', 'V']: \n",
    "        configs.append(\n",
    "            triton.testing.Benchmark(\n",
    "                x_names=[variable],  # Argument names to use as an x-axis for the plot.\n",
    "                x_vals=[(2**i)//f for i in range_dict[variable]],  # Different possible values for `x_name`.\n",
    "                x_log=True,  # x axis is logarithmic.\n",
    "                line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "                line_vals=['torch', 'torch-compile', 'torch-compile-checkpoint', 'triton', 'triton-recomp', 'triton-par-recomp', 'triton-many-recomp', 'malek'],\n",
    "                line_names=['torch', 'torch-compile', 'torch-compile-checkpoint', 'triton', 'triton-recomp', 'triton-par-recomp', 'triton-many-recomp', 'malek'],\n",
    "                ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "                plot_name=f'{mode}-Linear+Loss Performance. Defaults: N=B*S=16384, H=2048, V=131072',\n",
    "                args={\"mode\": mode},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "            ))\n",
    "        \n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(H=2048//f, V=131072//f, N=(4096 * 4)//f, provider=\"torch\", mode=\"fwd\"):\n",
    "    print(provider, N, H, V, mode)\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.float16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.float16)\n",
    "    At = A.detach().clone().T.contiguous()\n",
    "    At.requires_grad_()\n",
    "\n",
    "    if provider == 'torch':\n",
    "        fn = lambda: baseline_torch(x, y, A)\n",
    "    if provider == 'torch-compile':\n",
    "        fn = lambda: compiled_baseline(x, y, A)\n",
    "    if provider == \"torch-compile-checkpoint\":\n",
    "        fn = lambda: torch_compiled_checkpoint(x, y, A)\n",
    "    if provider == \"triton\":\n",
    "        fn = lambda: linear_cross_entropy(x, y, At)\n",
    "    if provider == \"triton-nolock-nowrite\":\n",
    "        fn = lambda: linear_cross_entropy_nolock(x, y, At)\n",
    "    if provider == \"triton\":\n",
    "        fn = lambda: linear_cross_entropy_highmem(x, y, At)\n",
    "    if provider == \"triton-recomp\":\n",
    "        fn = lambda: linear_cross_entropy_double_recomp(x, y, At)\n",
    "    if provider == \"triton-par-recomp\":\n",
    "        fn = lambda: linear_cross_entropy_parallel_recomp(x, y, At)\n",
    "    if provider == \"malek\":\n",
    "        fn = lambda: efficient_xent(x, y, A)\n",
    "    if provider == \"triton-many-recomp\":\n",
    "        fn = lambda: linear_cross_entropy_manyway(x, y, At)\n",
    "\n",
    "    try:\n",
    "        if mode == \"fwd\":\n",
    "            @torch.no_grad\n",
    "            def test_fn():\n",
    "                fn()\n",
    "        elif mode == \"bwd\":\n",
    "            loss = fn()\n",
    "            test_fn = lambda: loss.backward(retain_graph=True)\n",
    "        elif mode == \"fwd-bwd\":\n",
    "            test_fn = lambda: fn().backward()\n",
    "        else:\n",
    "            test_fn = fn\n",
    "\n",
    "        quantiles = [0.5, 0.2, 0.8]\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(test_fn, quantiles=quantiles, warmup = 50, rep = 200)\n",
    "    except: # in any failure case\n",
    "        print(f\"error when computing {provider} for N={N}, H={H}, V={V}\")\n",
    "        ms, min_ms, max_ms = 1e6, 1e6, 1e6\n",
    "\n",
    "    flop = 2 * (N * H * V) + 3 * N * V\n",
    "    if mode == \"bwd\":\n",
    "        flop *= 2\n",
    "    if mode == \"fwd-bwd\":\n",
    "        flop *= 3\n",
    "    \n",
    "    perf = lambda ms: flop * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=False, show_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_with_memory_reporting(func, quantiles, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats(device=device)\n",
    "    initial_memory = torch.cuda.memory_allocated(device=device)\n",
    "    \n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: func(*args, **kwargs), quantiles=quantiles, warmup = 5, rep = 1)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device=device)\n",
    "    memory_used = peak_memory - initial_memory\n",
    "    \n",
    "    return ms, min_ms, max_ms, memory_used\n",
    "\n",
    "range_dict = {'H':range(8, 14, 1),'V': range(10, 18, 1),'N': range(8, 15, 1)}\n",
    "\n",
    "configs = []\n",
    "for mode in [\"fwd\", \"fwd-bwd\"]:\n",
    "    for variable in ['H', 'N', 'V']:\n",
    "        configs.append(\n",
    "            triton.testing.Benchmark(\n",
    "                x_names=[variable],  # Argument names to use as an x-axis for the plot.\n",
    "                x_vals=[2**i for i in range_dict[variable]],  # Different possible values for `x_name`.\n",
    "                x_log=True,  # x axis is logarithmic.\n",
    "                line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "                ylabel='Peak Memory in GB (excluding inputs)',  # Label name for the y-axis.\n",
    "                line_vals=['torch', 'torch-compile', 'torch-compile-checkpoint', 'triton', 'triton-recomp', 'triton-par-recomp', 'triton-many-recomp', 'malek'],\n",
    "                line_names=['torch', 'torch-compile', 'torch-compile-checkpoint', 'triton', 'triton-recomp', 'triton-par-recomp', 'triton-many-recomp', 'malek'],\n",
    "                args={\"mode\": mode},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "            ))\n",
    "        \n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(H=4096, V=131072, N=4096 * 4, provider=\"torch\", mode=\"fwd\"):\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    if provider == 'torch':\n",
    "        fn = lambda: baseline_torch(x, y, A)\n",
    "    if provider == 'torch-compile':\n",
    "        fn = lambda: compiled_baseline(x, y, A)\n",
    "    if provider == \"torch-compile-checkpoint\":\n",
    "        fn = lambda: torch_compiled_checkpoint(x, y, At)\n",
    "    if provider == \"triton\":\n",
    "        fn = lambda: linear_cross_entropy(x, y, At)\n",
    "\n",
    "    if mode == \"bwd\":\n",
    "        loss = fn()\n",
    "        fn = lambda: loss.backward(retain_graph=True)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    ms, min_ms, max_ms, max_memory_allocated = benchmark_with_memory_reporting(fn, quantiles=quantiles)\n",
    "\n",
    "    return max_memory_allocated / 1024**3, 0, 0\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
