{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_torch(x, y, A):\n",
    "    V = A.shape[0]\n",
    "    return F.cross_entropy(F.linear(x, A).view(-1, V).float(), y.view(-1))\n",
    "\n",
    "compiled_baseline = torch.compile(baseline_torch)\n",
    "maxauto_baseline = torch.compile(baseline_torch, fullgraph=True, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "   configs=[\n",
    "    triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}),\n",
    "   ],\n",
    "   key=['V', 'N', 'H'],\n",
    "   reset_to_zero=[\"loss_ptr\", \"m_global_ptr\", \"s_global_ptr\"]\n",
    ")\n",
    "@triton.jit\n",
    "def linear_xent_fwd_kernel_matmul(x_ptr,\n",
    "                y_ptr,\n",
    "                A_ptr,\n",
    "                loss_ptr,  \n",
    "                m_global_ptr,\n",
    "                s_global_ptr,\n",
    "                stride_x_N, stride_x_H,\n",
    "                stride_A_V, stride_A_H,\n",
    "                V: tl.constexpr, N: tl.constexpr, H: tl.constexpr,\n",
    "                V_BLOCK_SIZE: tl.constexpr,\n",
    "                N_BLOCK_SIZE: tl.constexpr,\n",
    "                H_BLOCK_SIZE: tl.constexpr,\n",
    "               ):\n",
    "    idx = tl.program_id(axis=0)  \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_ptr,\n",
    "        shape=(V, H),\n",
    "        strides=(stride_A_V, stride_A_H),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(V_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    v_range = tl.arange(0, V_BLOCK_SIZE)\n",
    "    y = tl.load(y_ptr + offsets)\n",
    "\n",
    "    m = tl.load(m_global_ptr + offsets)\n",
    "    s = tl.load(s_global_ptr + offsets)\n",
    "    loss = 0.\n",
    "\n",
    "    \n",
    "    \n",
    "    for _ in range(V // V_BLOCK_SIZE):\n",
    "        \n",
    "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        for _ in range(H // H_BLOCK_SIZE):\n",
    "            x_chunk = tl.load(local_x_block_ptr) # Nc x H \n",
    "            A_v = tl.load(A_block_ptr) # Vc x H \n",
    "\n",
    "            z_j_to_k = tl.dot(x_chunk, A_v.trans(), z_j_to_k) # (Nc x H) @ (H x Vc)\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            A_block_ptr = tl.advance(A_block_ptr, [0, H_BLOCK_SIZE])\n",
    "    \n",
    "            \n",
    "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n",
    "\n",
    "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n",
    "        s = s * tl.exp(m - m_new) + s_update\n",
    "\n",
    "        mask = y[:, None] == v_range[None, :] # Nc x Vc\n",
    "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n",
    "\n",
    "        m = m_new\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [V_BLOCK_SIZE, -H_BLOCK_SIZE * (H // H_BLOCK_SIZE)])\n",
    "        v_range = v_range + V_BLOCK_SIZE\n",
    "    \n",
    "    # tl.device_print(\"m\", m)\n",
    "    # tl.device_print(\"s\", s)\n",
    "    loss += tl.sum(m + tl.log(s)) / N\n",
    "\n",
    "    tl.atomic_add(loss_ptr, loss)\n",
    "    tl.store(m_global_ptr + offsets, m)\n",
    "    tl.store(s_global_ptr + offsets, s)\n",
    "\n",
    "def linear_xent_matmul(x, y, A):\n",
    "    N, H = x.shape\n",
    "    V, H_A = A.shape\n",
    "    assert H_A == H\n",
    "    assert y.shape == (N,)\n",
    "    x = x.contiguous()\n",
    "    y = y.contiguous()\n",
    "    A = A.contiguous()\n",
    "\n",
    "    assert V % 256 == 0, f\"V is {V}\"\n",
    "    assert N % 64 == 0, f\"N is {N}\"\n",
    "    assert H % 64 == 0, f\"H is {H}\"\n",
    "\n",
    "    m_global = -10e5 * torch.ones(N, dtype=torch.float32, device=x.device)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32, device=x.device)\n",
    "    loss = torch.zeros(1, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # grid = (num_blocks,)\n",
    "    grid = lambda meta: (triton.cdiv(N, meta['N_BLOCK_SIZE']), )\n",
    "\n",
    "    with torch.cuda.device(x.device.index): # actually required\n",
    "        linear_xent_fwd_kernel_matmul[grid](\n",
    "                x,\n",
    "                y,\n",
    "                A,\n",
    "                loss,  \n",
    "                m_global,\n",
    "                s_global,\n",
    "                x.stride(0), x.stride(1),\n",
    "                A.stride(0), A.stride(1),\n",
    "                V=V, N=N, H=H)\n",
    "    # print(linear_xent_fwd_kernel_matmul.best_config)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "   configs=[\n",
    "    triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}),\n",
    "    triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}),\n",
    "   ],\n",
    "   key=['V', 'N', 'H'],\n",
    "   reset_to_zero=[\"loss_ptr\", \"m_global_ptr\", \"s_global_ptr\"]\n",
    ")\n",
    "@triton.jit\n",
    "def linear_xent_fwd_kernel_matmul_t(x_ptr,\n",
    "                y_ptr,\n",
    "                A_t_ptr,\n",
    "                loss_ptr,  \n",
    "                m_global_ptr,\n",
    "                s_global_ptr,\n",
    "                stride_x_N, stride_x_H,\n",
    "                stride_A_H, stride_A_V,\n",
    "                V: tl.constexpr, N: tl.constexpr, H: tl.constexpr,\n",
    "                V_BLOCK_SIZE: tl.constexpr,\n",
    "                N_BLOCK_SIZE: tl.constexpr,\n",
    "                H_BLOCK_SIZE: tl.constexpr,\n",
    "               ):\n",
    "    idx = tl.program_id(axis=0)  \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_t_ptr,\n",
    "        shape=(H, V),\n",
    "        strides=(stride_A_H, stride_A_V),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    v_range = tl.arange(0, V_BLOCK_SIZE)\n",
    "    y = tl.load(y_ptr + offsets)\n",
    "\n",
    "    m = tl.load(m_global_ptr + offsets)\n",
    "    s = tl.load(s_global_ptr + offsets)\n",
    "    loss = 0.\n",
    "\n",
    "    \n",
    "    \n",
    "    for _ in range(V // V_BLOCK_SIZE):\n",
    "        \n",
    "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        for _ in range(H // H_BLOCK_SIZE):\n",
    "            x_chunk = tl.load(local_x_block_ptr) # Nc x H \n",
    "            A_v = tl.load(A_block_ptr) # Vc x H \n",
    "\n",
    "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k) # (Nc x H) @ (H x Vc)\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "    \n",
    "            \n",
    "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n",
    "\n",
    "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n",
    "        s = s * tl.exp(m - m_new) + s_update\n",
    "\n",
    "        mask = y[:, None] == v_range[None, :] # Nc x Vc\n",
    "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n",
    "\n",
    "        m = m_new\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n",
    "        v_range = v_range + V_BLOCK_SIZE\n",
    "    \n",
    "    # tl.device_print(\"m\", m)\n",
    "    # tl.device_print(\"s\", s)\n",
    "    loss += tl.sum(m + tl.log(s)) / N\n",
    "\n",
    "    tl.atomic_add(loss_ptr, loss)\n",
    "    tl.store(m_global_ptr + offsets, m)\n",
    "    tl.store(s_global_ptr + offsets, s)\n",
    "\n",
    "@torch.no_grad\n",
    "def linear_xent_matmul_At(x, y, At):\n",
    "    N, H = x.shape\n",
    "    H_A, V = At.shape # V, H_A = A.shape\n",
    "    assert H_A == H\n",
    "    assert y.shape == (N,)\n",
    "    x = x.contiguous()\n",
    "    y = y.contiguous()\n",
    "    At = At.contiguous()\n",
    "\n",
    "    assert V % 256 == 0, f\"V is {V}\"\n",
    "    assert N % 64 == 0, f\"N is {N}\"\n",
    "    assert H % 64 == 0, f\"H is {H}\"\n",
    "\n",
    "    m_global = -10e5 * torch.ones(N, dtype=torch.float32, device=x.device)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32, device=x.device)\n",
    "    loss = torch.zeros(1, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # grid = (num_blocks,)\n",
    "    grid = lambda meta: (triton.cdiv(N, meta['N_BLOCK_SIZE']), )\n",
    "\n",
    "    with torch.cuda.device(x.device.index): # actually required\n",
    "        linear_xent_fwd_kernel_matmul_t[grid](\n",
    "                x,\n",
    "                y,\n",
    "                At,\n",
    "                loss,  \n",
    "                m_global,\n",
    "                s_global,\n",
    "                x.stride(0), x.stride(1),\n",
    "                At.stride(0), At.stride(1),\n",
    "                V=V, N=N, H=H)\n",
    "    # print(linear_xent_fwd_kernel_matmul.best_config)\n",
    "    return loss, m_global, s_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "   configs=[\n",
    "    # triton.Config({'N_BLOCK_SIZE': 1}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 2}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 4}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 8}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 16}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 32}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 64}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 128}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 256}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 512}),\n",
    "\n",
    "   ],\n",
    "   key=['N'],\n",
    "   restore_value=[\"loss_ptr\", \"m_global_ptr\", \"s_global_ptr\"]\n",
    ")\n",
    "@triton.jit\n",
    "def linear_xent_fwd_kernel(x_ptr,\n",
    "                y_ptr,\n",
    "                A_ptr,\n",
    "                loss_ptr,  \n",
    "                m_global_ptr,\n",
    "                s_global_ptr,\n",
    "                stride_x_N, stride_x_H,\n",
    "                stride_A_V, stride_A_H,\n",
    "                V: tl.constexpr, N: tl.constexpr, H: tl.constexpr,\n",
    "                N_BLOCK_SIZE: tl.constexpr,\n",
    "               ):\n",
    "    idx = tl.program_id(axis=0)  \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_ptr,\n",
    "        shape=(V, H),\n",
    "        strides=(stride_A_V, stride_A_H),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(1, H),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    y = tl.load(y_ptr + offsets)\n",
    "    m = tl.load(m_global_ptr + offsets)\n",
    "    s = tl.load(s_global_ptr + offsets)\n",
    "    # log2_const = 1.4426950408889634 not precise enough with exp2 \n",
    "    loss = 0.\n",
    "\n",
    "    x_chunk = tl.load(x_block_ptr) # Nc x H \n",
    "    \n",
    "    for v in range(V):\n",
    "        A_v = tl.load(A_block_ptr) # Vc x H\n",
    "        z_j = tl.sum((x_chunk * A_v).to(tl.float32), axis=1) # (Nc x H) @ (H x 1)\n",
    "\n",
    "        m_new = tl.maximum(m, z_j)\n",
    "        s = s * tl.exp(m - m_new) + tl.exp(z_j - m_new)\n",
    "        loss -= tl.sum(tl.where(y == v, z_j, 0.))\n",
    "\n",
    "        m = m_new\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [1, 0])\n",
    "    \n",
    "    loss = (loss + tl.sum(m + tl.log(s))) / N\n",
    "\n",
    "    tl.atomic_add(loss_ptr, loss)\n",
    "    tl.store(m_global_ptr + offsets, m)\n",
    "    tl.store(s_global_ptr + offsets, s)\n",
    "\n",
    "\n",
    "# loss_triton = linear_cross_entropy(x, y, A) # type: ignore\n",
    "# loss_triton, torch.dist(reference_loss, loss_triton).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def fwd_kernel(\n",
    "    x_ND_ptr,\n",
    "    w_DV_ptr,\n",
    "    c_N_ptr,\n",
    "    output_N_ptr,\n",
    "    l_N_ptr,\n",
    "    N, D, V,\n",
    "    stride_xn, stride_xd,\n",
    "    stride_wd, stride_wv,\n",
    "    # Meta-parameters\n",
    "    BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr, BLOCK_V: tl.constexpr,\n",
    "):\n",
    "    # TODO: more parallelism, e.g. tiled softmax \n",
    "    #       w/ parallelization across tiles (intra-tile computation uses online softmax)\n",
    "    # TODO: mask\n",
    "    # only parallelize along the N dimension\n",
    "    # i is the same as n\n",
    "    i = tl.program_id(axis=0)\n",
    "    offs_n_bN = i * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    c_i_bN = tl.load(c_N_ptr + offs_n_bN)\n",
    "    output_i_bN = tl.zeros([BLOCK_N], dtype=tl.float32)\n",
    "\n",
    "    # statistics for online softmax\n",
    "    m_i_bN = tl.zeros([BLOCK_N], dtype=tl.float32) - float('inf')\n",
    "    l_i_bN = tl.zeros([BLOCK_N], dtype=tl.float32) + 1.0\n",
    "    for start_v in range(0, V, BLOCK_V):\n",
    "        start_v = tl.multiple_of(start_v, BLOCK_V)\n",
    "        offs_v_bN = start_v + tl.arange(0, BLOCK_V)\n",
    "        # TODO: mask\n",
    "        x_ND_block_ptr = tl.make_block_ptr(\n",
    "            base=x_ND_ptr,\n",
    "            shape=(N, D),\n",
    "            strides=(stride_xn, stride_xd),\n",
    "            offsets=(i * BLOCK_N, 0),\n",
    "            block_shape=(BLOCK_N, BLOCK_D),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        w_DV_block_ptr = tl.make_block_ptr(\n",
    "            base=w_DV_ptr,\n",
    "            shape=(D, V),\n",
    "            strides=(stride_wd, stride_wv),\n",
    "            offsets=(0, start_v),\n",
    "            block_shape=(BLOCK_D, BLOCK_V),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        xw_bNbV = tl.zeros([BLOCK_N, BLOCK_V], dtype=tl.float32)\n",
    "        for start_d in range(0, D, BLOCK_D):\n",
    "            start_d = tl.multiple_of(start_d, BLOCK_D)\n",
    "            # TODO: mask\n",
    "            # TODO: x load can be reduced?\n",
    "            x_bNbD = tl.load(x_ND_block_ptr)\n",
    "            w_bDbV = tl.load(w_DV_block_ptr)\n",
    "            xw_bNbV = tl.dot(x_bNbD, w_bDbV, xw_bNbV)\n",
    "            x_ND_block_ptr = tl.advance(x_ND_block_ptr, (0, BLOCK_D))\n",
    "            w_DV_block_ptr = tl.advance(w_DV_block_ptr, (BLOCK_D, 0))\n",
    "        \n",
    "        # i for N\n",
    "        # j for V\n",
    "        m_ij_bN = tl.maximum(m_i_bN, tl.max(xw_bNbV, axis=1))\n",
    "        p_ij_bNbV = tl.exp(xw_bNbV - m_ij_bN[:, None])\n",
    "        l_ij_bN = tl.sum(p_ij_bNbV, axis=1)\n",
    "        # update m_i and l_i\n",
    "        alpha_bN = tl.exp(m_i_bN - m_ij_bN)\n",
    "        l_i_bN = l_i_bN * alpha_bN + l_ij_bN\n",
    "        m_i_bN = m_ij_bN\n",
    "        # update output\n",
    "        p_ic_bN = tl.sum(tl.where(c_i_bN[:, None] == offs_v_bN[None, :], p_ij_bNbV, 0.0), axis=1)\n",
    "        output_i_bN = output_i_bN * alpha_bN + p_ic_bN\n",
    "\n",
    "    output_i_bN = tl.log(output_i_bN) - tl.log(l_i_bN)\n",
    "    tl.store(output_N_ptr + offs_n_bN, output_i_bN)\n",
    "    tl.store(l_N_ptr + offs_n_bN, l_i_bN)\n",
    "\n",
    "\n",
    "# output_N[n] = log_softmax(x_ND @ w_DV, dim=1)[n, c_N[n]]\n",
    "class LMHeadThenLogSoftmaxThenGather(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_ND: torch.Tensor, w_DV: torch.Tensor, c_N: torch.Tensor):\n",
    "        # TODO\n",
    "        N, D = x_ND.shape\n",
    "        Dw, V = w_DV.shape\n",
    "        Nc, = c_N.shape\n",
    "        assert D == Dw and N == Nc\n",
    "        output_N = x_ND.new_empty(N)\n",
    "        l_N = x_ND.new_empty(N)\n",
    "        BLOCK_N = 32\n",
    "        BLOCK_D = 32\n",
    "        BLOCK_V = 32\n",
    "        grid = (triton.cdiv(N, BLOCK_N),)\n",
    "        fwd_kernel[grid](\n",
    "            x_ND, w_DV, c_N,\n",
    "            output_N, l_N,\n",
    "            N, D, V,\n",
    "            x_ND.stride(0), x_ND.stride(1),\n",
    "            w_DV.stride(0), w_DV.stride(1),\n",
    "            BLOCK_N, BLOCK_D, BLOCK_V,\n",
    "        )\n",
    "        ctx.save_for_backward(w_DV, x_ND, c_N, l_N)\n",
    "        return output_N\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g_NV):\n",
    "        # TODO\n",
    "        w_DV, x_ND, c_N, l_N = ctx.saved_tensors\n",
    "\n",
    "def YouJiacheng_linear_xent(x, y, At):\n",
    "    return LMHeadThenLogSoftmaxThenGather.apply(x, At, y).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCrossEntropyLoss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x,\n",
    "        y,\n",
    "        A,\n",
    "        ignore_index=-100, # ignores all negative integers ...\n",
    "    ):\n",
    "        N, H = x.shape\n",
    "        V, H_A = A.shape\n",
    "        assert H_A == H\n",
    "        assert y.shape == (N,)\n",
    "        x = x.contiguous()\n",
    "        y = y.contiguous()\n",
    "        A = A.contiguous()\n",
    "\n",
    "        # V_BLOCK_SIZE = 16\n",
    "        # N_BLOCK_SIZE = 16 # 64\n",
    "\n",
    "        assert N % 16 == 0\n",
    "        # assert V % V_BLOCK_SIZE == 0\n",
    "\n",
    "        # num_blocks = N // N_BLOCK_SIZE\n",
    "        m_global = -10e5 * torch.ones(N, dtype=torch.float32, device=x.device)\n",
    "        s_global = torch.zeros(N, dtype=torch.float32, device=x.device)\n",
    "        loss = torch.zeros(1, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # grid = (num_blocks,)\n",
    "        grid = lambda meta: (triton.cdiv(N, meta['N_BLOCK_SIZE']), )\n",
    "        with torch.cuda.device(x.device.index): # actually required\n",
    "            linear_xent_fwd_kernel[grid](\n",
    "                    x,\n",
    "                    y,\n",
    "                    A,\n",
    "                    loss,  \n",
    "                    m_global,\n",
    "                    s_global,\n",
    "                    x.stride(0), x.stride(1),\n",
    "                    A.stride(0), A.stride(1),\n",
    "                    V=V, N=N, H=H,\n",
    "                )\n",
    "        # print(linear_xent_fwd_kernel.best_config)\n",
    "\n",
    "\n",
    "        ctx.save_for_backward(m_global, s_global)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, losses):\n",
    "        pass\n",
    "        return x, None, A\n",
    "    \n",
    "def linear_cross_entropy(x, y, A):\n",
    "    return LinearCrossEntropyLoss.apply(x, y, A)\n",
    "\n",
    "# loss_triton = linear_cross_entropy(x, y, A) # type: ignore\n",
    "# loss_triton, torch.dist(reference_loss, loss_triton).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_with_memory_reporting(func, quantiles, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats(device=device)\n",
    "    initial_memory = torch.cuda.memory_allocated(device=device)\n",
    "    \n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: func(*args, **kwargs), quantiles=quantiles)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device=device)\n",
    "    memory_used = peak_memory - initial_memory\n",
    "    \n",
    "    return ms, min_ms, max_ms, memory_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1 # number from 1 to 512 to get fast results\n",
    "default_H = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['H'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 14, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t', 'triton-variant'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t', 'triton-variant'],  # Label name for the lines.\n",
    "        # styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('brown', '-')],  # Line styles.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(H, provider):\n",
    "    B, S, V = 4, 4096 // f, 131072 // f\n",
    "    N = B * S \n",
    "    H = H // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    if provider == \"triton-variant\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: YouJiacheng_linear_xent(x, y, At), quantiles=quantiles)\n",
    "        \n",
    "    perf = lambda ms: 2 * (N * H * V) * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(10, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t',\"triton-variant\"],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t',\"triton-variant\"],  # Label name for the lines.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    B, S , H = 4, 4096//f, default_H//f\n",
    "    N = B * S \n",
    "    V = V // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    if provider == \"triton-variant\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: YouJiacheng_linear_xent(x, y, At), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * N * H * V * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 15, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t',\"triton-variant\"],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t',\"triton-variant\"],  # Label name for the lines.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(N, provider):\n",
    "    H, V = default_H//f, 131072//f\n",
    "    N = N // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    if provider == \"triton-variant\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: YouJiacheng_linear_xent(x, y, At), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * N * H * V * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 15, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='GBs of Memory',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(N, provider):\n",
    "    H, V = default_H//f, 131072//f\n",
    "    N = N // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    return memory_used / 1024**3, 0, 0\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(10, 19, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='GBs of Memory',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark(V, provider):\n",
    "    B, S , H = 4, 4096//f, default_H//f\n",
    "    N = B * S \n",
    "    V = V // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    return memory_used / 1024**3, 0, 0\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_stream(torch.cuda.Stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 15, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(N, provider):\n",
    "    H, V = default_H//f, 131072//f\n",
    "    N = N // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: baseline_torch(x, y, A))\n",
    "    if provider == 'torch-compile':\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: compiled_baseline(x, y, A))\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: maxauto_baseline(x, y, A))\n",
    "    if provider == \"triton\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_cross_entropy(x, y, A))\n",
    "    if provider == \"triton2\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_xent_matmul(x, y, A))\n",
    "    if provider == \"triton2\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_xent_matmul(x, y, A))\n",
    "    if provider == \"triton2-t\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_xent_matmul_At(x, y, At))\n",
    "    perf = lambda ms: 2 * N * H * V * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
