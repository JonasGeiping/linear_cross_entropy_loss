{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(x,y):\n",
    "    return ((x.reshape(-1).double() * y.reshape(-1).double()).sum() / x.reshape(-1).double().norm() / y.reshape(-1).double().norm()).float()\n",
    "\n",
    "def baseline_torch(x, y, A):\n",
    "    V = A.shape[0]\n",
    "    return F.cross_entropy(F.linear(x, A).view(-1, V).float(), y.view(-1))\n",
    "\n",
    "compiled_baseline = torch.compile(baseline_torch)\n",
    "maxauto_baseline = torch.compile(baseline_torch, fullgraph=True, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S , H, V = 4, 4 // f, 16 // f, 512 // f\n",
    "N = B * S \n",
    "\n",
    "y = torch.randint(0, V, (B * S,), device=device) # vocab ** B S \n",
    "A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "At = A.clone().T.contiguous()\n",
    "\n",
    "# x = torch.randn(B * S, H, requires_grad=True, device=device, dtype=torch.float32) # B S H\n",
    "# x = A[y].clone().detach()\n",
    "x = 0.05 * A[y].clone().detach() + torch.randn(B * S, H, device=device, dtype=torch.bfloat16)\n",
    "x.requires_grad_()\n",
    "\n",
    "loss = baseline_torch(x.double(), y, A.double())\n",
    "loss.backward()\n",
    "\n",
    "reference_A_grad = A.grad.float().clone()\n",
    "reference_x_grad = x.grad.float().clone()\n",
    "reference_loss = loss.detach().float().clone()\n",
    "\n",
    "\n",
    "chunk_size = 16\n",
    "V_chunk_size = 16\n",
    "print(reference_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "   configs=[\n",
    "    triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}),\n",
    "   ],\n",
    "   key=['V', 'N', 'H'],\n",
    "   reset_to_zero=[\"loss_ptr\", \"m_global_ptr\", \"s_global_ptr\"]\n",
    ")\n",
    "@triton.jit\n",
    "def linear_xent_fwd_kernel_matmul(x_ptr,\n",
    "                y_ptr,\n",
    "                A_ptr,\n",
    "                loss_ptr,  \n",
    "                m_global_ptr,\n",
    "                s_global_ptr,\n",
    "                stride_x_N, stride_x_H,\n",
    "                stride_A_V, stride_A_H,\n",
    "                V: tl.constexpr, N: tl.constexpr, H: tl.constexpr,\n",
    "                V_BLOCK_SIZE: tl.constexpr,\n",
    "                N_BLOCK_SIZE: tl.constexpr,\n",
    "                H_BLOCK_SIZE: tl.constexpr,\n",
    "               ):\n",
    "    idx = tl.program_id(axis=0)  \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_ptr,\n",
    "        shape=(V, H),\n",
    "        strides=(stride_A_V, stride_A_H),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(V_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    v_range = tl.arange(0, V_BLOCK_SIZE)\n",
    "    y = tl.load(y_ptr + offsets)\n",
    "\n",
    "    m = tl.load(m_global_ptr + offsets)\n",
    "    s = tl.load(s_global_ptr + offsets)\n",
    "    loss = 0.\n",
    "\n",
    "    \n",
    "    \n",
    "    for _ in range(V // V_BLOCK_SIZE):\n",
    "        \n",
    "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        for _ in range(H // H_BLOCK_SIZE):\n",
    "            x_chunk = tl.load(local_x_block_ptr) # Nc x H \n",
    "            A_v = tl.load(A_block_ptr) # Vc x H \n",
    "\n",
    "            z_j_to_k = tl.dot(x_chunk, A_v.trans(), z_j_to_k) # (Nc x H) @ (H x Vc)\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            A_block_ptr = tl.advance(A_block_ptr, [0, H_BLOCK_SIZE])\n",
    "    \n",
    "            \n",
    "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n",
    "\n",
    "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n",
    "        s = s * tl.exp(m - m_new) + s_update\n",
    "\n",
    "        mask = y[:, None] == v_range[None, :] # Nc x Vc\n",
    "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n",
    "\n",
    "        m = m_new\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [V_BLOCK_SIZE, -H_BLOCK_SIZE * (H // H_BLOCK_SIZE)])\n",
    "        v_range = v_range + V_BLOCK_SIZE\n",
    "    \n",
    "    # tl.device_print(\"m\", m)\n",
    "    # tl.device_print(\"s\", s)\n",
    "    loss += tl.sum(m + tl.log(s)) / N\n",
    "\n",
    "    tl.atomic_add(loss_ptr, loss)\n",
    "    tl.store(m_global_ptr + offsets, m)\n",
    "    tl.store(s_global_ptr + offsets, s)\n",
    "\n",
    "def linear_xent_matmul(x, y, A):\n",
    "    N, H = x.shape\n",
    "    V, H_A = A.shape\n",
    "    assert H_A == H\n",
    "    assert y.shape == (N,)\n",
    "    x = x.contiguous()\n",
    "    y = y.contiguous()\n",
    "    A = A.contiguous()\n",
    "\n",
    "    # assert V % 256 == 0, f\"V is {V}\"\n",
    "    # assert N % 64 == 0, f\"N is {N}\"\n",
    "    # assert H % 64 == 0, f\"H is {H}\"\n",
    "\n",
    "    m_global = -10e5 * torch.ones(N, dtype=torch.float32, device=x.device)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32, device=x.device)\n",
    "    loss = torch.zeros(1, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # grid = (num_blocks,)\n",
    "    grid = lambda meta: (triton.cdiv(N, meta['N_BLOCK_SIZE']), )\n",
    "\n",
    "    with torch.cuda.device(x.device.index): # actually required\n",
    "        linear_xent_fwd_kernel_matmul[grid](\n",
    "                x,\n",
    "                y,\n",
    "                A,\n",
    "                loss,  \n",
    "                m_global,\n",
    "                s_global,\n",
    "                x.stride(0), x.stride(1),\n",
    "                A.stride(0), A.stride(1),\n",
    "                V=V, N=N, H=H)\n",
    "    # print(linear_xent_fwd_kernel_matmul.best_config)\n",
    "    return loss\n",
    "\n",
    "loss = linear_xent_matmul(x, y, A) # autotune\n",
    "torch.cuda.synchronize()\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "loss = linear_xent_matmul(x, y, A)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "estimate_ms = start_event.elapsed_time(end_event)\n",
    "print(f\"Simple timing: {estimate_ms}ms\")\n",
    "print(loss), print(torch.dist(loss, reference_loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "   configs=[\n",
    "    triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}),\n",
    "    # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}),\n",
    "   ],\n",
    "   key=['V', 'N', 'H'],\n",
    "   reset_to_zero=[\"loss_ptr\", \"m_global_ptr\", \"s_global_ptr\"]\n",
    ")\n",
    "@triton.jit\n",
    "def linear_xent_fwd_kernel_matmul_t(x_ptr,\n",
    "                y_ptr,\n",
    "                A_t_ptr,\n",
    "                loss_ptr,  \n",
    "                m_global_ptr,\n",
    "                s_global_ptr,\n",
    "                stride_x_N, stride_x_H,\n",
    "                stride_A_H, stride_A_V,\n",
    "                V: tl.constexpr, N: tl.constexpr, H: tl.constexpr,\n",
    "                V_BLOCK_SIZE: tl.constexpr,\n",
    "                N_BLOCK_SIZE: tl.constexpr,\n",
    "                H_BLOCK_SIZE: tl.constexpr,\n",
    "               ):\n",
    "    idx = tl.program_id(axis=0)  \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_t_ptr,\n",
    "        shape=(H, V),\n",
    "        strides=(stride_A_H, stride_A_V),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    v_range = tl.arange(0, V_BLOCK_SIZE)\n",
    "    y = tl.load(y_ptr + offsets)\n",
    "\n",
    "    m = tl.load(m_global_ptr + offsets)\n",
    "    s = tl.load(s_global_ptr + offsets)\n",
    "    loss = 0.\n",
    "\n",
    "    for _ in range(V // V_BLOCK_SIZE):\n",
    "        \n",
    "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        for _ in range(H // H_BLOCK_SIZE):\n",
    "            x_chunk = tl.load(local_x_block_ptr) # Nc x H \n",
    "            A_v = tl.load(A_block_ptr) # Vc x H \n",
    "\n",
    "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k) # (Nc x H) @ (H x Vc)\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "    \n",
    "            \n",
    "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n",
    "\n",
    "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n",
    "        s = s * tl.exp(m - m_new) + s_update\n",
    "\n",
    "        mask = y[:, None] == v_range[None, :] # Nc x Vc\n",
    "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n",
    "\n",
    "        m = m_new\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n",
    "        v_range = v_range + V_BLOCK_SIZE\n",
    "    \n",
    "    # tl.device_print(\"m\", m)\n",
    "    # tl.device_print(\"s\", s)\n",
    "    loss += tl.sum(m + tl.log(s)) / N\n",
    "\n",
    "    tl.atomic_add(loss_ptr, loss)\n",
    "    tl.store(m_global_ptr + offsets, m)\n",
    "    tl.store(s_global_ptr + offsets, s)\n",
    "\n",
    "@torch.no_grad\n",
    "def linear_xent_matmul_At(x, y, At):\n",
    "    N, H = x.shape\n",
    "    H_A, V = At.shape # V, H_A = A.shape\n",
    "    assert H_A == H\n",
    "    assert y.shape == (N,)\n",
    "    x = x.contiguous()\n",
    "    y = y.contiguous()\n",
    "    At = At.contiguous()\n",
    "\n",
    "    # assert V % 256 == 0, f\"V is {V}\"\n",
    "    # assert N % 64 == 0, f\"N is {N}\"\n",
    "    # assert H % 64 == 0, f\"H is {H}\"\n",
    "\n",
    "    m_global = -10e5 * torch.ones(N, dtype=torch.float32, device=x.device)\n",
    "    s_global = torch.ones(N, dtype=torch.float32, device=x.device)\n",
    "    loss = torch.zeros(1, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # grid = (num_blocks,)\n",
    "    grid = lambda meta: (triton.cdiv(N, meta['N_BLOCK_SIZE']), )\n",
    "\n",
    "    with torch.cuda.device(x.device.index): # actually required\n",
    "        linear_xent_fwd_kernel_matmul_t[grid](\n",
    "                x,\n",
    "                y,\n",
    "                At,\n",
    "                loss,  \n",
    "                m_global,\n",
    "                s_global,\n",
    "                x.stride(0), x.stride(1),\n",
    "                At.stride(0), At.stride(1),\n",
    "                V=V, N=N, H=H)\n",
    "    # print(linear_xent_fwd_kernel_matmul.best_config)\n",
    "    return loss, m_global, s_global\n",
    "\n",
    "loss, _, _ = linear_xent_matmul_At(x, y, At) # autotune\n",
    "torch.cuda.synchronize()\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "loss, m_global, s_global = linear_xent_matmul_At(x, y, At)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "estimate_ms = start_event.elapsed_time(end_event)\n",
    "print(f\"Simple timing: {estimate_ms}ms\")\n",
    "print(loss), print(torch.dist(loss, reference_loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2(x, y, A):\n",
    "    x = x.cpu()\n",
    "    A = A.cpu()\n",
    "    y = y.cpu()\n",
    "\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N, dtype=torch.float32)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32)\n",
    "    losses = torch.zeros(num_chunks, dtype=torch.float32)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    \n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "        s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "        v_offsets = torch.arange(0, V_chunk_size)\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks):\n",
    "            m_prev = m.clone()\n",
    "            z_j_to_k = (x_chunk @ A_v.T).to(dtype=torch.float32)\n",
    "            m = torch.maximum(m_prev, z_j_to_k.max(dim=-1)[0])\n",
    "            s = s * (m_prev - m).exp() + (z_j_to_k-  m[:, None]).exp().sum(dim=-1)\n",
    "\n",
    "            mask = y_chunk[:, None] == v_offsets[None, :]\n",
    "            losses[idx] -= torch.where(mask, z_j_to_k, 0.0).sum() / N\n",
    "\n",
    "            v_offsets += V_chunk_size\n",
    "\n",
    "\n",
    "        losses[idx] += (m + s.log()).sum() / N\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return losses.sum(), m_global, s_global\n",
    "\n",
    "loss, m_ref2, s_ref2 = manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2(x, y, A)\n",
    "loss.item(), torch.dist(loss, reference_loss).item() # , f\"{torch.dist(loss, reference_loss).item():2.4e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = F.linear(x, A).view(-1, V).float()\n",
    "torch.dist(m_global, z.max(dim=1)[0]), cosim(z.max(dim=1)[0], m_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dist(s_global, (z - m_global[:, None]).exp().sum(dim=1)), cosim(s_global, (z - m_global[:, None]).exp().sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(z.max(dim=1)[0] / m_global).mean(), 1 / (s_global / (z - m_global[:, None]).exp().sum(dim=1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dist(m_ref2, z.cpu().max(dim=1)[0]), torch.dist(s_ref2, (z.cpu() - m_ref2[:, None]).exp().sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_chunk_size = 16\n",
    "V_chunk_size = 16\n",
    "N_chunk_size = 16\n",
    "print(N_chunk_size, V_chunk_size, H_chunk_size)\n",
    "print(N, V, H)\n",
    "\n",
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH_At(x, y, A, m_global, s_global):\n",
    "    compute_dtype = torch.float32\n",
    "    x = x.to(dtype=compute_dtype).cpu() # float16 is much more accurate for \"sane\" values of x and A\n",
    "    At = A.T.to(dtype=compute_dtype).cpu()\n",
    "    y = y.cpu()\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(N_chunk_size)\n",
    "    y_chunks = y.view(-1).split(N_chunk_size)\n",
    "\n",
    "    At_chunks = At.split(V_chunk_size, dim=1)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    Atgrad = torch.zeros_like(At, dtype=torch.float32)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * N_chunk_size: (idx+1)*N_chunk_size]\n",
    "        m = m_global[idx * N_chunk_size: (idx+1)*N_chunk_size]\n",
    "        # xgrad = torch.zeros_like(x_chunk)\n",
    "        Nc = x_chunk.shape[0]\n",
    "        Vc = At_chunks[0].shape[1]\n",
    "\n",
    "        for v_idx, A_v in enumerate(At_chunks): # can parallelize\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "\n",
    "            z_j_to_k = torch.zeros(Nc, Vc)\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "                z_j_to_k += (x_chunk_h @ A_chunk_h).to(dtype=torch.float32)\n",
    "            \n",
    "            softmax_z = ((z_j_to_k - m[:, None]).exp() / s[:, None]).to(dtype=compute_dtype)\n",
    "            mask = (y_chunk[:, None] == v_range[None, :])[:,:,None] # needs to be N_BLOCK_SIZE x V_BLOCK_SIZE x 1 ?\n",
    "\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "\n",
    "                xgrad_temp = (softmax_z @  A_chunk_h.T).to(dtype=torch.float32)\n",
    "                xgrad_temp -= torch.where(mask, A_chunk_h.T[None, :, :], 0.0).sum(dim=1)\n",
    "                global_x_grad[idx * N_chunk_size: (idx+1)*N_chunk_size, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size] += xgrad_temp / N\n",
    "                Agrad_temp = (softmax_z.T @ x_chunk_h).to(dtype=torch.float32)\n",
    "                Agrad_temp -= torch.where(mask, x_chunk_h[:, None, :], 0.0).sum(dim=0)\n",
    "                Atgrad[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, v_range] += Agrad_temp.T\n",
    "               \n",
    "    \n",
    "    return (Atgrad / N).T, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH_At(x, y, A, m_ref2, s_ref2)\n",
    "torch.dist(reference_x_grad.cpu(), xgrad), cosim(reference_x_grad.cpu(), xgrad), torch.dist(reference_A_grad.cpu(), Agrad), cosim(reference_A_grad.cpu(), Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @triton.autotune(\n",
    "#    configs=[\n",
    "#     triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}, num_warps=1, num_ctas=1, num_stages=1),\n",
    "#     # triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "#     # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}),\n",
    "#     # triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}),\n",
    "#     # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}),\n",
    "\n",
    "#     # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}),\n",
    "#     # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}),\n",
    "#     # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}),\n",
    "#    ],\n",
    "#    key=['V', 'N', 'H'],\n",
    "#    reset_to_zero=[\"A_grad_ptr\", \"x_grad_ptr\"]\n",
    "# )\n",
    "@triton.jit()\n",
    "def linear_xent_bwd_kernel_matmul_t(x_ptr,\n",
    "                y_ptr,\n",
    "                A_t_ptr,\n",
    "                m_global_ptr,\n",
    "                s_global_ptr,\n",
    "                A_grad_ptr,\n",
    "                x_grad_ptr,\n",
    "                locks_N_ptr,\n",
    "                locks_V_ptr,\n",
    "                stride_x_N, stride_x_H,\n",
    "                stride_A_H, stride_A_V,\n",
    "                stride_x_grad_N, stride_x_grad_H,\n",
    "                stride_A_grad_H, stride_A_grad_V,\n",
    "                V: tl.constexpr, N: tl.constexpr, H: tl.constexpr,\n",
    "                V_BLOCK_SIZE: tl.constexpr = 16,\n",
    "                N_BLOCK_SIZE: tl.constexpr = 16,\n",
    "                H_BLOCK_SIZE: tl.constexpr = 16,\n",
    "               ):\n",
    "    idx_N = tl.program_id(axis=0)\n",
    "    idx_V = tl.program_id(axis=1)\n",
    "\n",
    "    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n",
    "    \n",
    "    y = tl.load(y_ptr + offsets)\n",
    "    m = tl.load(m_global_ptr + offsets)\n",
    "    s = tl.load(s_global_ptr + offsets)\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx_N * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    x_grad_block_ptr = tl.make_block_ptr(\n",
    "        base=x_grad_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_grad_N, stride_x_grad_H),\n",
    "        offsets=(idx_N * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_t_ptr,\n",
    "        shape=(H, V),\n",
    "        strides=(stride_A_H, stride_A_V),\n",
    "        offsets=(0, idx_V * V_BLOCK_SIZE),\n",
    "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    A_grad_block_ptr = tl.make_block_ptr(\n",
    "        base=A_grad_ptr,\n",
    "        shape=(H, V),\n",
    "        strides=(stride_A_grad_H, stride_A_grad_V),\n",
    "        offsets=(0, idx_V * V_BLOCK_SIZE),\n",
    "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "\n",
    "    local_x_block_ptr = x_block_ptr\n",
    "    local_A_block_ptr = A_block_ptr\n",
    "    for _ in range(H // H_BLOCK_SIZE):\n",
    "        x_chunk = tl.load(local_x_block_ptr) # Nc x Hc \n",
    "        A_v = tl.load(A_block_ptr) # Vc x Hc \n",
    "\n",
    "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k) # (Nc x Hc) @ (Hc x Vc)\n",
    "\n",
    "        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "        local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "\n",
    "    mask = (y[:, None] == v_range[None, :])[:, :, None] # N_BLOCK_SIZE x V_BLOCK_SIZE x 1\n",
    "    # the reason for the double loop\n",
    "    softmax_z = ((z_j_to_k - m[:, None]).exp() / s[:, None])\n",
    "\n",
    "    for _ in range(H // H_BLOCK_SIZE):\n",
    "        x_chunk = tl.load(x_block_ptr).to(tl.float32) # Nc x Hc \n",
    "        A_v = tl.load(A_block_ptr).to(tl.float32) # Vc x Hc \n",
    "        \n",
    "        # xgrad\n",
    "        temp_xgrad = tl.dot(softmax_z, A_v.trans())\n",
    "        temp_xgrad -= tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1)\n",
    "\n",
    "        # Lock in V direction for x accumulation\n",
    "        # tl.atomic_add(x_grad_block_ptr, temp_xgrad)\n",
    "        while tl.atomic_cas(locks_V_ptr + idx_V, 0, 1) == 1:\n",
    "            pass\n",
    "        temp_xgrad = temp_xgrad / N + tl.load(x_grad_block_ptr)\n",
    "        tl.store(x_grad_block_ptr, temp_xgrad)\n",
    "        tl.atomic_xchg(locks_V_ptr + idx_V, 0)\n",
    "\n",
    "        # Agrad\n",
    "        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n",
    "        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n",
    "        temp_Agrad = temp_Agrad.trans() # to T\n",
    "\n",
    "        # Lock in N direction for A accumulation\n",
    "        # tl.atomic_add(A_grad_block_ptr, temp_Agrad)\n",
    "        while tl.atomic_cas(locks_N_ptr + idx_N, 0, 1) == 1:\n",
    "            pass\n",
    "        temp_Agrad = temp_Agrad / N + tl.load(A_grad_block_ptr)\n",
    "\n",
    "        tl.store(A_grad_block_ptr, temp_Agrad)\n",
    "        tl.atomic_xchg(locks_N_ptr + idx_N, 0)\n",
    " \n",
    "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "        x_grad_block_ptr = tl.advance(x_grad_block_ptr, [0, H_BLOCK_SIZE])\n",
    "\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "    \n",
    "\n",
    "@torch.no_grad\n",
    "def linear_xent_matmul_bwd(x, y, At, m_global, s_global):\n",
    "    N, H = x.shape\n",
    "    H_A, V = At.shape # V, H_A = A.shape\n",
    "    assert H_A == H\n",
    "    assert y.shape == (N,)\n",
    "    x = x.contiguous()\n",
    "    y = y.contiguous()\n",
    "    At = At.contiguous()\n",
    "\n",
    "    # assert V % 256 == 0, f\"V is {V}\"\n",
    "    # assert N % 64 == 0, f\"N is {N}\"\n",
    "    # assert H % 64 == 0, f\"H is {H}\"\n",
    "\n",
    "    xgrad = torch.zeros_like(x, dtype=torch.float32)\n",
    "    Atgrad = torch.zeros_like(At, dtype=torch.float32)\n",
    "\n",
    "    # grid = (num_blocks,)\n",
    "    grid = lambda meta: (triton.cdiv(N, meta['N_BLOCK_SIZE']), triton.cdiv(V, meta['V_BLOCK_SIZE']))\n",
    "    locks_N = torch.zeros(N // 16, dtype=torch.int32, device=x.device) # use minimal block sizes for now, how to make this dynamic?\n",
    "    locks_V = torch.zeros(V // 16, dtype=torch.int32, device=x.device)\n",
    "\n",
    "    with torch.cuda.device(x.device.index): # actually required\n",
    "        linear_xent_bwd_kernel_matmul_t[grid](\n",
    "                x,\n",
    "                y,\n",
    "                At,\n",
    "                m_global.contiguous(),\n",
    "                s_global.contiguous(),\n",
    "                Atgrad,\n",
    "                xgrad,\n",
    "                locks_N, locks_V,\n",
    "                x.stride(0), x.stride(1),\n",
    "                At.stride(0), At.stride(1),\n",
    "                xgrad.stride(0), xgrad.stride(1),\n",
    "                Atgrad.stride(0), Atgrad.stride(1),\n",
    "                V=V, N=N, H=H)\n",
    "    # print(linear_xent_bwd_kernel_matmul_t.best_config)\n",
    "    return xgrad, Atgrad \n",
    "\n",
    "xgrad, Atgrad = linear_xent_matmul_bwd(x, y, At, m_ref2.to(device), s_ref2.to(device))\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Atgrad.T), cosim(reference_A_grad, Atgrad.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrad.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrad.min(), reference_x_grad.min(), torch.dist(reference_x_grad, xgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "   configs=[\n",
    "    # triton.Config({'N_BLOCK_SIZE': 1}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 2}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 4}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 8}),\n",
    "    triton.Config({'N_BLOCK_SIZE': 16}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 32}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 64}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 128}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 256}),\n",
    "    # triton.Config({'N_BLOCK_SIZE': 512}),\n",
    "\n",
    "   ],\n",
    "   key=['N'],\n",
    "   restore_value=[\"loss_ptr\", \"m_global_ptr\", \"s_global_ptr\"]\n",
    ")\n",
    "@triton.jit\n",
    "def linear_xent_fwd_kernel(x_ptr,\n",
    "                y_ptr,\n",
    "                A_ptr,\n",
    "                loss_ptr,  \n",
    "                m_global_ptr,\n",
    "                s_global_ptr,\n",
    "                stride_x_N, stride_x_H,\n",
    "                stride_A_V, stride_A_H,\n",
    "                V: tl.constexpr, N: tl.constexpr, H: tl.constexpr,\n",
    "                N_BLOCK_SIZE: tl.constexpr,\n",
    "               ):\n",
    "    idx = tl.program_id(axis=0)  \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_ptr,\n",
    "        shape=(V, H),\n",
    "        strides=(stride_A_V, stride_A_H),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(1, H),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    y = tl.load(y_ptr + offsets)\n",
    "    m = tl.load(m_global_ptr + offsets)\n",
    "    s = tl.load(s_global_ptr + offsets)\n",
    "    # log2_const = 1.4426950408889634 not precise enough with exp2 \n",
    "    loss = 0.\n",
    "\n",
    "    x_chunk = tl.load(x_block_ptr) # Nc x H \n",
    "    \n",
    "    for v in range(V):\n",
    "        A_v = tl.load(A_block_ptr) # Vc x H\n",
    "        z_j = tl.sum((x_chunk * A_v).to(tl.float32), axis=1) # (Nc x H) @ (H x 1)\n",
    "\n",
    "        m_new = tl.maximum(m, z_j)\n",
    "        s = s * tl.exp(m - m_new) + tl.exp(z_j - m_new)\n",
    "        loss -= tl.sum(tl.where(y == v, z_j, 0.))\n",
    "\n",
    "        m = m_new\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [1, 0])\n",
    "    \n",
    "    loss = (loss + tl.sum(m + tl.log(s))) / N\n",
    "\n",
    "    tl.atomic_add(loss_ptr, loss)\n",
    "    tl.store(m_global_ptr + offsets, m)\n",
    "    tl.store(s_global_ptr + offsets, s)\n",
    "\n",
    "\n",
    "# loss_triton = linear_cross_entropy(x, y, A) # type: ignore\n",
    "# loss_triton, torch.dist(reference_loss, loss_triton).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def fwd_kernel(\n",
    "    x_ND_ptr,\n",
    "    w_DV_ptr,\n",
    "    c_N_ptr,\n",
    "    output_N_ptr,\n",
    "    l_N_ptr,\n",
    "    N, D, V,\n",
    "    stride_xn, stride_xd,\n",
    "    stride_wd, stride_wv,\n",
    "    # Meta-parameters\n",
    "    BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr, BLOCK_V: tl.constexpr,\n",
    "):\n",
    "    # TODO: more parallelism, e.g. tiled softmax \n",
    "    #       w/ parallelization across tiles (intra-tile computation uses online softmax)\n",
    "    # TODO: mask\n",
    "    # only parallelize along the N dimension\n",
    "    # i is the same as n\n",
    "    i = tl.program_id(axis=0)\n",
    "    offs_n_bN = i * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    c_i_bN = tl.load(c_N_ptr + offs_n_bN)\n",
    "    output_i_bN = tl.zeros([BLOCK_N], dtype=tl.float32)\n",
    "\n",
    "    # statistics for online softmax\n",
    "    m_i_bN = tl.zeros([BLOCK_N], dtype=tl.float32) - float('inf')\n",
    "    l_i_bN = tl.zeros([BLOCK_N], dtype=tl.float32) + 1.0\n",
    "    for start_v in range(0, V, BLOCK_V):\n",
    "        start_v = tl.multiple_of(start_v, BLOCK_V)\n",
    "        offs_v_bN = start_v + tl.arange(0, BLOCK_V)\n",
    "        # TODO: mask\n",
    "        x_ND_block_ptr = tl.make_block_ptr(\n",
    "            base=x_ND_ptr,\n",
    "            shape=(N, D),\n",
    "            strides=(stride_xn, stride_xd),\n",
    "            offsets=(i * BLOCK_N, 0),\n",
    "            block_shape=(BLOCK_N, BLOCK_D),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        w_DV_block_ptr = tl.make_block_ptr(\n",
    "            base=w_DV_ptr,\n",
    "            shape=(D, V),\n",
    "            strides=(stride_wd, stride_wv),\n",
    "            offsets=(0, start_v),\n",
    "            block_shape=(BLOCK_D, BLOCK_V),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        xw_bNbV = tl.zeros([BLOCK_N, BLOCK_V], dtype=tl.float32)\n",
    "        for start_d in range(0, D, BLOCK_D):\n",
    "            start_d = tl.multiple_of(start_d, BLOCK_D)\n",
    "            # TODO: mask\n",
    "            # TODO: x load can be reduced?\n",
    "            x_bNbD = tl.load(x_ND_block_ptr)\n",
    "            w_bDbV = tl.load(w_DV_block_ptr)\n",
    "            xw_bNbV = tl.dot(x_bNbD, w_bDbV, xw_bNbV)\n",
    "            x_ND_block_ptr = tl.advance(x_ND_block_ptr, (0, BLOCK_D))\n",
    "            w_DV_block_ptr = tl.advance(w_DV_block_ptr, (BLOCK_D, 0))\n",
    "        \n",
    "        # i for N\n",
    "        # j for V\n",
    "        m_ij_bN = tl.maximum(m_i_bN, tl.max(xw_bNbV, axis=1))\n",
    "        p_ij_bNbV = tl.exp(xw_bNbV - m_ij_bN[:, None])\n",
    "        l_ij_bN = tl.sum(p_ij_bNbV, axis=1)\n",
    "        # update m_i and l_i\n",
    "        alpha_bN = tl.exp(m_i_bN - m_ij_bN)\n",
    "        l_i_bN = l_i_bN * alpha_bN + l_ij_bN\n",
    "        m_i_bN = m_ij_bN\n",
    "        # update output\n",
    "        p_ic_bN = tl.sum(tl.where(c_i_bN[:, None] == offs_v_bN[None, :], p_ij_bNbV, 0.0), axis=1)\n",
    "        output_i_bN = output_i_bN * alpha_bN + p_ic_bN\n",
    "\n",
    "    output_i_bN = tl.log(output_i_bN) - tl.log(l_i_bN)\n",
    "    tl.store(output_N_ptr + offs_n_bN, output_i_bN)\n",
    "    tl.store(l_N_ptr + offs_n_bN, l_i_bN)\n",
    "\n",
    "\n",
    "# output_N[n] = log_softmax(x_ND @ w_DV, dim=1)[n, c_N[n]]\n",
    "class LMHeadThenLogSoftmaxThenGather(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_ND: torch.Tensor, w_DV: torch.Tensor, c_N: torch.Tensor):\n",
    "        # TODO\n",
    "        N, D = x_ND.shape\n",
    "        Dw, V = w_DV.shape\n",
    "        Nc, = c_N.shape\n",
    "        assert D == Dw and N == Nc\n",
    "        output_N = x_ND.new_empty(N)\n",
    "        l_N = x_ND.new_empty(N)\n",
    "        BLOCK_N = 32\n",
    "        BLOCK_D = 32\n",
    "        BLOCK_V = 32\n",
    "        grid = (triton.cdiv(N, BLOCK_N),)\n",
    "        fwd_kernel[grid](\n",
    "            x_ND, w_DV, c_N,\n",
    "            output_N, l_N,\n",
    "            N, D, V,\n",
    "            x_ND.stride(0), x_ND.stride(1),\n",
    "            w_DV.stride(0), w_DV.stride(1),\n",
    "            BLOCK_N, BLOCK_D, BLOCK_V,\n",
    "        )\n",
    "        ctx.save_for_backward(w_DV, x_ND, c_N, l_N)\n",
    "        return output_N\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g_NV):\n",
    "        # TODO\n",
    "        w_DV, x_ND, c_N, l_N = ctx.saved_tensors\n",
    "\n",
    "def YouJiacheng_linear_xent(x, y, At):\n",
    "    return LMHeadThenLogSoftmaxThenGather.apply(x, At, y).sum()\n",
    "\n",
    "\n",
    "loss = YouJiacheng_linear_xent(x, y, At)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCrossEntropyLoss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x,\n",
    "        y,\n",
    "        A,\n",
    "        ignore_index=-100, # ignores all negative integers ...\n",
    "    ):\n",
    "        N, H = x.shape\n",
    "        V, H_A = A.shape\n",
    "        assert H_A == H\n",
    "        assert y.shape == (N,)\n",
    "        x = x.contiguous()\n",
    "        y = y.contiguous()\n",
    "        A = A.contiguous()\n",
    "\n",
    "        # V_BLOCK_SIZE = 16\n",
    "        # N_BLOCK_SIZE = 16 # 64\n",
    "\n",
    "        assert N % 16 == 0\n",
    "        # assert V % V_BLOCK_SIZE == 0\n",
    "\n",
    "        # num_blocks = N // N_BLOCK_SIZE\n",
    "        m_global = -10e5 * torch.ones(N, dtype=torch.float32, device=x.device)\n",
    "        s_global = torch.zeros(N, dtype=torch.float32, device=x.device)\n",
    "        loss = torch.zeros(1, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # grid = (num_blocks,)\n",
    "        grid = lambda meta: (triton.cdiv(N, meta['N_BLOCK_SIZE']), )\n",
    "        with torch.cuda.device(x.device.index): # actually required\n",
    "            linear_xent_fwd_kernel[grid](\n",
    "                    x,\n",
    "                    y,\n",
    "                    A,\n",
    "                    loss,  \n",
    "                    m_global,\n",
    "                    s_global,\n",
    "                    x.stride(0), x.stride(1),\n",
    "                    A.stride(0), A.stride(1),\n",
    "                    V=V, N=N, H=H,\n",
    "                )\n",
    "        # print(linear_xent_fwd_kernel.best_config)\n",
    "\n",
    "\n",
    "        ctx.save_for_backward(m_global, s_global)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, losses):\n",
    "        pass\n",
    "        return x, None, A\n",
    "    \n",
    "def linear_cross_entropy(x, y, A):\n",
    "    return LinearCrossEntropyLoss.apply(x, y, A)\n",
    "\n",
    "loss_triton = linear_cross_entropy(x, y, A) # type: ignore\n",
    "loss_triton, torch.dist(reference_loss, loss_triton).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_with_memory_reporting(func, quantiles, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats(device=device)\n",
    "    initial_memory = torch.cuda.memory_allocated(device=device)\n",
    "    \n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: func(*args, **kwargs), quantiles=quantiles)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device=device)\n",
    "    memory_used = peak_memory - initial_memory\n",
    "    \n",
    "    return ms, min_ms, max_ms, memory_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make space for benchmarking\n",
    "del A\n",
    "del At\n",
    "del x\n",
    "del y\n",
    "del reference_loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1 # number from 1 to 512 to get fast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['H'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 14, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t', 'triton-variant'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t', 'triton-variant'],  # Label name for the lines.\n",
    "        # styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('brown', '-')],  # Line styles.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(H, provider):\n",
    "    B, S, V = 4, 4096 // f, 131072 // f\n",
    "    N = B * S \n",
    "    H = H // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    if provider == \"triton-variant\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: YouJiacheng_linear_xent(x, y, At), quantiles=quantiles)\n",
    "        \n",
    "    perf = lambda ms: 2 * N * H * V * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(10, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    B, S , H = 4, 4096//f, 4096//f\n",
    "    N = B * S \n",
    "    V = V // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * N * H * V * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 15, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(N, provider):\n",
    "    H, V = 4096//f, 131072//f\n",
    "    N = N // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "\n",
    "    perf = lambda ms: 2 * N * H * V * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 15, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='GBs of Memory',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(N, provider):\n",
    "    H, V = 4096//f, 131072//f\n",
    "    N = N // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    return memory_used / 1024**3, 0, 0\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(10, 19, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='GBs of Memory',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark(V, provider):\n",
    "    B, S , H = 4, 4096//f, 4096//f\n",
    "    N = B * S \n",
    "    V = V // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: baseline_torch(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: compiled_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: maxauto_baseline(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_cross_entropy(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul(x, y, A), quantiles=quantiles)\n",
    "    if provider == \"triton2-t\":\n",
    "        ms, min_ms, max_ms, memory_used = benchmark_with_memory_reporting(lambda: linear_xent_matmul_At(x, y, At), quantiles=quantiles)\n",
    "    return memory_used / 1024**3, 0, 0\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_stream(torch.cuda.Stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(9, 15, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Possible values for `line_arg`.\n",
    "        line_names=['torch', 'torch-compile', 'triton2', 'triton2-t'],  # Label name for the lines.\n",
    "        ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "        plot_name='Linear+Loss Performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(N, provider):\n",
    "    H, V = 4096//f, 131072//f\n",
    "    N = N // f\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: baseline_torch(x, y, A))\n",
    "    if provider == 'torch-compile':\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: compiled_baseline(x, y, A))\n",
    "    if provider == 'torch-maxauto':\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: maxauto_baseline(x, y, A))\n",
    "    if provider == \"triton\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_cross_entropy(x, y, A))\n",
    "    if provider == \"triton2\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_xent_matmul(x, y, A))\n",
    "    if provider == \"triton2\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_xent_matmul(x, y, A))\n",
    "    if provider == \"triton2-t\":\n",
    "        ms = triton.testing.do_bench_cudagraph(lambda: linear_xent_matmul_At(x, y, At))\n",
    "    perf = lambda ms: 2 * N * H * V * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
