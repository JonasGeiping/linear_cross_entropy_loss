{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(x,y):\n",
    "    return ((x.reshape(-1).double() * y.reshape(-1).double()).sum() / x.reshape(-1).double().norm() / y.reshape(-1).double().norm()).float()\n",
    "\n",
    "@torch._dynamo.disable\n",
    "def baseline_torch(x, y, A):\n",
    "    V = A.shape[0]\n",
    "    return F.cross_entropy(F.linear(x, A).view(-1, V).float(), y.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, N, H = 128, 128, 128\n",
    "\n",
    "compute_dtype = torch.float16\n",
    "\n",
    "y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "A = torch.randn(V, H, requires_grad=True, device=device, dtype=compute_dtype)\n",
    "At = A.clone().detach().T.contiguous()\n",
    "At.requires_grad_()\n",
    "\n",
    "# x = torch.randn(B * S, H, requires_grad=True, device=device, dtype=torch.float32) # B S H\n",
    "# x = A[y].clone().detach()\n",
    "x = 0.01 * A[y].clone().detach() + torch.randn(N, H, device=device, dtype=compute_dtype)\n",
    "x.requires_grad_()\n",
    "\n",
    "loss = baseline_torch(x.double(), y, A.double())\n",
    "loss.backward()\n",
    "\n",
    "reference_A_grad = A.grad.float().clone()\n",
    "reference_x_grad = x.grad.float().clone()\n",
    "reference_loss = loss.detach().float().clone()\n",
    "\n",
    "z_ref = F.linear(x, A).view(-1, V).float().detach()\n",
    "m_ref = z_ref.max(dim=1)[0]\n",
    "s_ref = (z_ref - m_ref[:, None]).exp().sum(dim=1)\n",
    "\n",
    "print(reference_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bench(x, y, At, fn, reference_loss, reference_x_grad, reference_A_grad):\n",
    "    x.grad, At.grad = None, None\n",
    "    loss_triton = fn(x, y, At) # warmup\n",
    "    torch.cuda.synchronize()\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    start_event.record()\n",
    "    loss_triton = fn(x, y, At)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    estimate_ms_fwd = start_event.elapsed_time(end_event)\n",
    "    print(f\"fwd : {estimate_ms_fwd}ms\")\n",
    "    print(f\"fwd error: {torch.dist(loss_triton, reference_loss).item()}\")\n",
    "\n",
    "    loss_triton.backward(retain_graph=True) # warmup\n",
    "    x.grad, At.grad = None, None\n",
    "    torch.cuda.synchronize()\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    start_event.record()\n",
    "    loss_triton.backward(retain_graph=True)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    estimate_ms_bwd = start_event.elapsed_time(end_event)\n",
    "    print(f\"bwd : {estimate_ms_bwd}ms\")\n",
    "    print(f\"bwd error: {torch.dist(reference_x_grad, x.grad).item()}, {torch.dist(reference_A_grad.T, At.grad).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}),\n",
    "\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 64, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 64}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 64, \"N_BLOCK_SIZE\": 64, \"H_BLOCK_SIZE\": 64}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 256}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 512, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 512}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 64, \"H_BLOCK_SIZE\": 64}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 256, \"H_BLOCK_SIZE\": 64}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 256, \"H_BLOCK_SIZE\": 256}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}),\n",
    "    ],\n",
    "    key=[\"V\", \"N\", \"H\"],\n",
    "    reset_to_zero=[\"losses_ptr\", \"lse_ptr\"],\n",
    ")\n",
    "@triton.jit\n",
    "def linear_xent_fwd_kernel_matmul_t(\n",
    "    x_ptr,\n",
    "    y_ptr,\n",
    "    A_t_ptr,\n",
    "    losses_ptr,\n",
    "    lse_ptr,\n",
    "    stride_x_N,\n",
    "    stride_x_H,\n",
    "    stride_A_H,\n",
    "    stride_A_V,\n",
    "    V: tl.constexpr,\n",
    "    N: tl.constexpr,\n",
    "    H: tl.constexpr,\n",
    "    V_BLOCK_SIZE: tl.constexpr,\n",
    "    N_BLOCK_SIZE: tl.constexpr,\n",
    "    H_BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    idx = tl.program_id(axis=0)\n",
    "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_t_ptr,\n",
    "        shape=(H, V),\n",
    "        strides=(stride_A_H, stride_A_V),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    v_range = tl.arange(0, V_BLOCK_SIZE)\n",
    "    y = tl.load(y_ptr + offsets)\n",
    "\n",
    "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n",
    "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n",
    "    loss = 0.0\n",
    "\n",
    "    for _ in range(V // V_BLOCK_SIZE):\n",
    "\n",
    "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        for _ in range(H // H_BLOCK_SIZE):\n",
    "            x_chunk = tl.load(local_x_block_ptr)  # Nc x H\n",
    "            A_v = tl.load(A_block_ptr)  # Vc x H\n",
    "\n",
    "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)  # (Nc x H) @ (H x Vc)\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "\n",
    "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n",
    "\n",
    "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n",
    "        s = s * tl.exp(m - m_new) + s_update\n",
    "\n",
    "        mask = y[:, None] == v_range[None, :]  # Nc x Vc\n",
    "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n",
    "\n",
    "        m = m_new\n",
    "        A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n",
    "        v_range = v_range + V_BLOCK_SIZE\n",
    "\n",
    "    lse = m + tl.log(s)\n",
    "    loss += tl.sum(lse) / N\n",
    "    tl.store(losses_ptr + idx, loss)\n",
    "    tl.store(lse_ptr + offsets, lse)\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}),\n",
    "        triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}, num_warps=1, num_ctas=1, num_stages=1),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 32, \"N_BLOCK_SIZE\": 32, \"H_BLOCK_SIZE\": 32}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 32, \"N_BLOCK_SIZE\": 32, \"H_BLOCK_SIZE\": 64}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 32, \"N_BLOCK_SIZE\": 64, \"H_BLOCK_SIZE\": 64}),\n",
    "        # # triton.Config({\"V_BLOCK_SIZE\": 64, \"N_BLOCK_SIZE\": 64, \"H_BLOCK_SIZE\": 64}), # incorrect result???\n",
    "        # # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 256, \"H_BLOCK_SIZE\": 16}), # slow\n",
    "        # # # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}),# out of addresses\n",
    "        # # # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}),# out of addresses\n",
    "        # # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}), # slow\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 128}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 256}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 512}),\n",
    "    ],\n",
    "    key=[\"V\", \"N\", \"H\"],\n",
    "    reset_to_zero=[\"A_grad_ptr\"],\n",
    ")\n",
    "@triton.jit()\n",
    "def linear_xent_bwd_kernel_matmul_t_dA(\n",
    "    x_ptr,\n",
    "    y_ptr,\n",
    "    A_t_ptr,\n",
    "    lse_global_ptr,\n",
    "    A_grad_ptr,\n",
    "    stride_x_N,\n",
    "    stride_x_H,\n",
    "    stride_A_H,\n",
    "    stride_A_V,\n",
    "    V: tl.constexpr,\n",
    "    N: tl.constexpr,\n",
    "    H: tl.constexpr,\n",
    "    V_BLOCK_SIZE: tl.constexpr = 16,\n",
    "    N_BLOCK_SIZE: tl.constexpr = 16,\n",
    "    H_BLOCK_SIZE: tl.constexpr = 16,\n",
    "):\n",
    "    idx_V = tl.program_id(axis=0)\n",
    "\n",
    "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n",
    "\n",
    "    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n",
    "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "    A_block_ptr = tl.make_block_ptr(\n",
    "        base=A_t_ptr,\n",
    "        shape=(H, V),\n",
    "        strides=(stride_A_H, stride_A_V),\n",
    "        offsets=(0, idx_V * V_BLOCK_SIZE),\n",
    "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    A_grad_block_ptr = tl.make_block_ptr(\n",
    "        base=A_grad_ptr,\n",
    "        shape=(H, V),\n",
    "        strides=(stride_A_H, stride_A_V),\n",
    "        offsets=(0, idx_V * V_BLOCK_SIZE),\n",
    "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    for idx_N in range(N // N_BLOCK_SIZE):\n",
    "        x_block_ptr = tl.make_block_ptr(\n",
    "            base=x_ptr,\n",
    "            shape=(N, H),\n",
    "            strides=(stride_x_N, stride_x_H),\n",
    "            offsets=(idx_N * N_BLOCK_SIZE, 0),\n",
    "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "\n",
    "        y = tl.load(y_ptr + N_offsets)\n",
    "        lse = tl.load(lse_global_ptr + N_offsets)\n",
    "\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        local_A_block_ptr = A_block_ptr\n",
    "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "        for _ in range(H // H_BLOCK_SIZE):\n",
    "            x_chunk = tl.load(local_x_block_ptr)  # Nc x Hc\n",
    "            A_v = tl.load(local_A_block_ptr)  # Hc x Vc\n",
    "\n",
    "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)  # (Nc x Hc) @ (Hc x Vc)\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "\n",
    "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]  # N_BLOCK_SIZE x V_BLOCK_SIZE x 1\n",
    "        # the reason for the double loop\n",
    "        softmax_z = (z_j_to_k - lse[:, None]).exp()  # computed Nc chunk that will be consumed for Agrad\n",
    "\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        local_A_block_ptr = A_block_ptr\n",
    "        local_A_grad_block_ptr = A_grad_block_ptr\n",
    "        for idx_H in range(H // H_BLOCK_SIZE):\n",
    "            A_grad_block_ptr = tl.make_block_ptr(\n",
    "                base=A_grad_ptr,\n",
    "                shape=(H, V),\n",
    "                strides=(stride_A_H, stride_A_V),\n",
    "                offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n",
    "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "                order=(1, 0),\n",
    "            )\n",
    "\n",
    "            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)  # Nc x Hc\n",
    "            A_v = tl.load(local_A_block_ptr).to(tl.float32)  # Hc x Vc\n",
    "\n",
    "            # Agrad\n",
    "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk).trans()\n",
    "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0).trans()\n",
    "            temp_AgradT = temp_Agrad / N + tl.load(A_grad_block_ptr) # to T\n",
    "            tl.debug_barrier()\n",
    "            # tl.store(A_grad_block_ptr, temp_AgradT, boundary_check=(0, 1))\n",
    "            tl.debug_barrier()\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            # A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "\n",
    "        # x_block_ptr = tl.advance(x_block_ptr, [0, N_BLOCK_SIZE])\n",
    "        N_offsets += N_BLOCK_SIZE\n",
    "        tl.debug_barrier()\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}),\n",
    "        triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}, num_warps=1, num_ctas=1, num_stages=1),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 32, \"N_BLOCK_SIZE\": 32, \"H_BLOCK_SIZE\": 32}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 32, \"N_BLOCK_SIZE\": 32, \"H_BLOCK_SIZE\": 64}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 32, \"N_BLOCK_SIZE\": 64, \"H_BLOCK_SIZE\": 64}),\n",
    "        # # triton.Config({\"V_BLOCK_SIZE\": 64, \"N_BLOCK_SIZE\": 64, \"H_BLOCK_SIZE\": 64}), # incorrect result???\n",
    "        # # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 256, \"H_BLOCK_SIZE\": 16}), # slow\n",
    "        # # # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}),# out of addresses\n",
    "        # # # triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}),# out of addresses\n",
    "        # # triton.Config({\"V_BLOCK_SIZE\": 256, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 16}), # slow\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 128}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 256}),\n",
    "        # triton.Config({\"V_BLOCK_SIZE\": 16, \"N_BLOCK_SIZE\": 16, \"H_BLOCK_SIZE\": 512}),\n",
    "    ],\n",
    "    key=[\"V\", \"N\", \"H\"],\n",
    "    reset_to_zero=[\"x_grad_ptr\"],\n",
    ")\n",
    "@triton.jit()\n",
    "def linear_xent_bwd_kernel_matmul_t_dx(\n",
    "    x_ptr,\n",
    "    y_ptr,\n",
    "    A_t_ptr,\n",
    "    lse_global_ptr,\n",
    "    x_grad_ptr,\n",
    "    stride_x_N,\n",
    "    stride_x_H,\n",
    "    stride_A_H,\n",
    "    stride_A_V,\n",
    "    V: tl.constexpr,\n",
    "    N: tl.constexpr,\n",
    "    H: tl.constexpr,\n",
    "    V_BLOCK_SIZE: tl.constexpr = 16,\n",
    "    N_BLOCK_SIZE: tl.constexpr = 16,\n",
    "    H_BLOCK_SIZE: tl.constexpr = 16,\n",
    "):\n",
    "    idx_N = tl.program_id(axis=0)\n",
    "\n",
    "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n",
    "\n",
    "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n",
    "    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=x_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx_N * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    x_grad_block_ptr = tl.make_block_ptr(\n",
    "        base=x_grad_ptr,\n",
    "        shape=(N, H),\n",
    "        strides=(stride_x_N, stride_x_H),\n",
    "        offsets=(idx_N * N_BLOCK_SIZE, 0),\n",
    "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    y = tl.load(y_ptr + N_offsets)\n",
    "    lse = tl.load(lse_global_ptr + N_offsets)\n",
    "\n",
    "    for idx_V in range(V // V_BLOCK_SIZE):\n",
    "        A_block_ptr = tl.make_block_ptr(\n",
    "            base=A_t_ptr,\n",
    "            shape=(H, V),\n",
    "            strides=(stride_A_H, stride_A_V),\n",
    "            offsets=(0, idx_V * V_BLOCK_SIZE),\n",
    "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        local_A_block_ptr = A_block_ptr\n",
    "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n",
    "        for _ in range(H // H_BLOCK_SIZE):\n",
    "            x_chunk = tl.load(local_x_block_ptr)  # Nc x Hc\n",
    "            A_v = tl.load(local_A_block_ptr)  # Hc x Vc\n",
    "\n",
    "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)  # (Nc x Hc) @ (Hc x Vc)\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "\n",
    "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]  # N_BLOCK_SIZE x V_BLOCK_SIZE x 1\n",
    "        # the reason for the double loop\n",
    "        softmax_z = (z_j_to_k - lse[:, None]).exp()  # computed Vc chunk that will be consumed for xgrad\n",
    "\n",
    "        local_x_block_ptr = x_block_ptr\n",
    "        local_A_block_ptr = A_block_ptr\n",
    "        local_x_grad_block_ptr = x_grad_block_ptr\n",
    "        for idx_H in range(H // H_BLOCK_SIZE):\n",
    "            # tl.device_print(\"H\", idx_H)\n",
    "            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)  # Nc x Hc\n",
    "            A_v = tl.load(local_A_block_ptr).to(tl.float32)  # Hc x Vc\n",
    "\n",
    "            # xgrad\n",
    "            temp_xgrad = tl.dot(softmax_z, A_v.trans()) / N\n",
    "            temp_xgrad -= tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1) / N\n",
    "\n",
    "            temp_xgrad += tl.load(local_x_grad_block_ptr)\n",
    "            tl.store(local_x_grad_block_ptr, temp_xgrad, boundary_check=(0, 1))\n",
    "\n",
    "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            local_x_grad_block_ptr = tl.advance(local_x_grad_block_ptr, [0, H_BLOCK_SIZE])\n",
    "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n",
    "            # tl.debug_barrier()\n",
    "        # A_block_ptr = tl.advance(A_block_ptr, [0, V_BLOCK_SIZE])\n",
    "        V_offsets += V_BLOCK_SIZE\n",
    "\n",
    "\n",
    "class LinearCrossEntropyLoss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x,\n",
    "        y,\n",
    "        At,\n",
    "        ignore_index=-100,  # ignores all negative integers ...\n",
    "    ):\n",
    "        N, H = x.shape\n",
    "        H_A, V = At.shape\n",
    "        assert H_A == H\n",
    "        assert y.shape == (N,)\n",
    "        x = x.contiguous()\n",
    "        y = y.contiguous()\n",
    "        At = At.contiguous()\n",
    "\n",
    "        assert V % 16 == 0, f\"V is {V}\"\n",
    "        assert N % 16 == 0, f\"N is {N}\"\n",
    "        assert H % 16 == 0, f\"H is {H}\"\n",
    "\n",
    "        lse_global = torch.zeros(N, dtype=torch.float32, device=x.device)\n",
    "        losses = torch.zeros(N // 16, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        grid = lambda meta: (triton.cdiv(N, meta[\"N_BLOCK_SIZE\"]),)\n",
    "\n",
    "        with torch.cuda.device(x.device.index):  # actually required\n",
    "            linear_xent_fwd_kernel_matmul_t[grid](\n",
    "                x, y, At, losses, lse_global, x.stride(0), x.stride(1), At.stride(0), At.stride(1), V=V, N=N, H=H\n",
    "            )\n",
    "        print(\"fwd config:\", linear_xent_fwd_kernel_matmul_t.best_config)\n",
    "\n",
    "        ctx.save_for_backward(x, y, At, lse_global)\n",
    "        \n",
    "        return losses.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, y, At, lse_global = ctx.saved_tensors\n",
    "        N, H = x.shape\n",
    "        _, V = At.shape\n",
    "\n",
    "        xgrad = torch.zeros_like(x, dtype=torch.float32)\n",
    "        Atgrad = torch.zeros_like(At, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        with torch.cuda.device(x.device.index):  # actually required\n",
    "            grid = lambda meta: (triton.cdiv(V, meta[\"V_BLOCK_SIZE\"]),)\n",
    "            linear_xent_bwd_kernel_matmul_t_dA[grid](\n",
    "                x,\n",
    "                y,\n",
    "                At,\n",
    "                lse_global,\n",
    "                Atgrad,\n",
    "                x.stride(0),\n",
    "                x.stride(1),\n",
    "                At.stride(0),\n",
    "                At.stride(1),\n",
    "                V=V,\n",
    "                N=N,\n",
    "                H=H,\n",
    "            )\n",
    "            print(\"bwd config dA:\", linear_xent_bwd_kernel_matmul_t_dA.best_config)\n",
    "            grid = lambda meta: (triton.cdiv(N, meta[\"N_BLOCK_SIZE\"]),)\n",
    "            linear_xent_bwd_kernel_matmul_t_dx[grid](\n",
    "                x,\n",
    "                y,\n",
    "                At,\n",
    "                lse_global,\n",
    "                xgrad,\n",
    "                x.stride(0),\n",
    "                x.stride(1),\n",
    "                At.stride(0),\n",
    "                At.stride(1),\n",
    "                V=V,\n",
    "                N=N,\n",
    "                H=H,\n",
    "            )\n",
    "            print(\"bwd config dx:\", linear_xent_bwd_kernel_matmul_t_dx.best_config)\n",
    "\n",
    "        ctx.mark_non_differentiable(y)\n",
    "        return xgrad * grad_output, None, Atgrad * grad_output, None\n",
    "\n",
    "\n",
    "def linear_cross_entropy(x, y, At):\n",
    "    return LinearCrossEntropyLoss.apply(x, y, At)\n",
    "\n",
    "\n",
    "simple_bench(x, y, At, linear_cross_entropy, reference_loss, reference_x_grad, reference_A_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, A, At"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try eventually:\n",
    "from litgpt.ops import linear_cross_entropy_checkerboard\n",
    "from litgpt.ops import linear_cross_entropy_nolock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking FWD + BWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 16 # make larger to let test go fast. f=1 is target size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_dict = {'H':range(9, 14, 1),'V': range(10, 18, 1),'N': range(8, 15, 1)}\n",
    "\n",
    "configs = []\n",
    "for mode in [\"bwd\"]: # \"fwd\", \n",
    "    for variable in ['N', 'V', 'H']: # 'H', \n",
    "        configs.append(\n",
    "            triton.testing.Benchmark(\n",
    "                x_names=[variable],  # Argument names to use as an x-axis for the plot.\n",
    "                x_vals=[(2**i)//f for i in range_dict[variable]],  # Different possible values for `x_name`.\n",
    "                x_log=True,  # x axis is logarithmic.\n",
    "                line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "                line_vals=['torch', 'torch-compile', \"triton-nolock\", \"triton-checkerboard\", 'triton'],\n",
    "                line_names=['torch', 'torch-compile', \"triton-nolock\", \"triton-checkerboard\", 'triton'],\n",
    "                ylabel='TFLOP/s',  # Label name for the y-axis.\n",
    "                plot_name=f'{mode}-Linear+Loss Performance. Defaults: N=B*S=16384, H=2048, V=131072',\n",
    "                args={\"mode\": mode},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "            ))\n",
    "        \n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(H=2048//f, V=131072//f, N=(4096 * 4)//f, provider=\"torch\", mode=\"fwd\"):\n",
    "    print(provider, H, V, N, mode)\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.detach().clone().T.contiguous()\n",
    "    At.requires_grad_()\n",
    "\n",
    "    if provider == 'torch':\n",
    "        fn = lambda: baseline_torch(x, y, A)\n",
    "    if provider == 'torch-compile':\n",
    "        fn = lambda: compiled_baseline(x, y, A)\n",
    "    if provider == \"torch-compile-checkpoint\":\n",
    "        fn = lambda: torch_compiled_checkpoint(x, y, A)\n",
    "    if provider == \"triton\":\n",
    "        fn = lambda: linear_cross_entropy(x, y, At)\n",
    "    if provider == \"triton-nolock\":\n",
    "        fn = lambda: linear_cross_entropy_nolock(x, y, At)\n",
    "    if provider == \"triton\":\n",
    "        fn = lambda: linear_cross_entropy(x, y, At)\n",
    "    if provider == \"triton-checkerboard\":\n",
    "        fn = lambda: linear_cross_entropy_checkerboard(x, y, At)\n",
    "\n",
    "    \n",
    "\n",
    "    if mode == \"bwd\":\n",
    "        loss = fn()\n",
    "        fn = lambda: loss.backward(retain_graph=True)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(fn, quantiles=quantiles, warmup = 25, rep = 100)\n",
    "\n",
    "    flop = 2 * (N * H * V) + 3 * N * V\n",
    "    if mode == \"bwd\":\n",
    "        flop *= 2\n",
    "    \n",
    "    perf = lambda ms: flop * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_with_memory_reporting(func, quantiles, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats(device=device)\n",
    "    initial_memory = torch.cuda.memory_allocated(device=device)\n",
    "    \n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: func(*args, **kwargs), quantiles=quantiles)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device=device)\n",
    "    memory_used = peak_memory - initial_memory\n",
    "    \n",
    "    return ms, min_ms, max_ms, memory_used\n",
    "\n",
    "range_dict = {'H':range(8, 14, 1),'V': range(10, 18, 1),'N': range(8, 15, 1)}\n",
    "\n",
    "configs = []\n",
    "for mode in [\"fwd\", \"bwd\"]:\n",
    "    for variable in ['H', 'N', 'V']:\n",
    "        configs.append(\n",
    "            triton.testing.Benchmark(\n",
    "                x_names=[variable],  # Argument names to use as an x-axis for the plot.\n",
    "                x_vals=[2**i for i in range_dict[variable]],  # Different possible values for `x_name`.\n",
    "                x_log=True,  # x axis is logarithmic.\n",
    "                line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "                line_vals=['torch', 'torch-compile', 'triton', \"torch-compile-checkpoint\"],  # Possible values for `line_arg`.\n",
    "                line_names=['torch', 'torch-compile', 'triton', \"torch-compile-checkpoint\"],  # Label name for the lines.\n",
    "                ylabel='Peak Memory in GB (excluding inputs)',  # Label name for the y-axis.\n",
    "                plot_name=f'{mode}-Linear+Loss Performance. Defaults: N=B*S=16384, H=4096, V=131072',\n",
    "                args={\"mode\": mode},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "            ))\n",
    "        \n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(H=4096, V=131072, N=4096 * 4, provider=\"torch\", mode=\"fwd\"):\n",
    "\n",
    "    x = torch.randn(N, H, requires_grad=True, device=device, dtype=torch.bfloat16) # B S H\n",
    "    y = torch.randint(0, V, (N,), device=device) # vocab ** B S \n",
    "    A = torch.randn(V, H, requires_grad=True, device=device, dtype=torch.bfloat16)\n",
    "    At = A.clone().T.contiguous()\n",
    "\n",
    "    if provider == 'torch':\n",
    "        fn = lambda: baseline_torch(x, y, A)\n",
    "    if provider == 'torch-compile':\n",
    "        fn = lambda: compiled_baseline(x, y, A)\n",
    "    if provider == \"torch-compile-checkpoint\":\n",
    "        fn = lambda: torch_compiled_checkpoint(x, y, At)\n",
    "    if provider == \"triton\":\n",
    "        fn = lambda: linear_cross_entropy(x, y, At)\n",
    "\n",
    "    if mode == \"bwd\":\n",
    "        loss = fn()\n",
    "        fn = lambda: loss.backward(retain_graph=True)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    ms, min_ms, max_ms, max_memory_allocated = benchmark_with_memory_reporting(fn, quantiles=quantiles, warmup = 5, rep = 1)\n",
    "\n",
    "    return memory_used / 1024**3, 0, 0\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
