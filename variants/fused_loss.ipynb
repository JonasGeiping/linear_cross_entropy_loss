{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test some fusion details for linear+xent (via online logsumexp and a kind of forward AD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "activities = [ProfilerActivity.CPU]\n",
    "\n",
    "# torch._logging.set_logs(dynamo=logging.DEBUG)\n",
    "# torch._logging.set_logs(graph=True)\n",
    "# torch._logging.set_logs(fusion=True)\n",
    "# torch._logging.set_logs(output_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(x,y):\n",
    "    return ((x.view(-1).double() * y.view(-1).double()).sum() / x.view(-1).double().norm() / y.view(-1).double().norm()).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S , H, V = 4, 512, 32, 4096\n",
    "N = B * S \n",
    "\n",
    "x = torch.randn(B, S, H, requires_grad=True) # B S H\n",
    "y = torch.randint(0, V, (B, S)) # vocab ** B S \n",
    "A = torch.randn(V, H, requires_grad=True)\n",
    "\n",
    "def baseline(x, y, A):\n",
    "    return F.cross_entropy(F.linear(x.double(), A.double()).view(-1, V), y.view(-1))\n",
    "\n",
    "loss = baseline(x, y, A)\n",
    "loss.backward()\n",
    "\n",
    "reference_A_grad = A.grad.float().clone()\n",
    "reference_x_grad = x.grad.float().clone()\n",
    "reference_loss = loss.detach().float().clone()\n",
    "\n",
    "loss.item(), A.grad.mean(), x.grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch compile version:\n",
    "chunk_size = 64 \n",
    "\n",
    "@torch.compile(mode=\"max-autotune\",fullgraph=True)\n",
    "def checkpointed_chunked(x, y, A):\n",
    "    loss = 0.\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    for x_chunk, y_chunk in zip(x_chunks, y_chunks):\n",
    "        loss += checkpoint(lambda A, x, y: F.cross_entropy(F.linear(x, A), y) / num_chunks, A, x_chunk, y_chunk,use_reentrant=False)\n",
    "    return loss\n",
    "\n",
    "loss = checkpointed_chunked(x, y, A)\n",
    "loss.item(), A.grad.mean(), x.grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_implementation_vectorized(x, y, A):\n",
    "    loss = 0.\n",
    "    x_cont = x.view(-1, H)\n",
    "    y_cont = y.view(-1)\n",
    "    # loss = (-(A[y_cont] * x_cont).sum(dim=-1) + z_nv.logsumexp(dim=-1)).mean()\n",
    "    z_nv = x_cont @ A.T\n",
    "    c_n = z_nv.max(dim=-1)[0]\n",
    "    loss = (-(A[y_cont] * x_cont).sum(dim=-1) + c_n + (z_nv - c_n[:, None]).exp().sum(dim=-1).log()).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "loss = manual_implementation_vectorized(x, y, A)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_implementation_chunked(x, y, A):\n",
    "    loss = 0.\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    # loss = (-(A[y_cont] * x_cont).sum(dim=-1) + z_nv.logsumexp(dim=-1)).mean()\n",
    "    for x_chunk, y_chunk in zip(x_chunks, y_chunks):\n",
    "        z_nv = x_chunk @ A.T\n",
    "        c_n = z_nv.max(dim=-1)[0]\n",
    "        loss += (-(A[y_chunk] * x_chunk).sum(dim=-1) + c_n + (z_nv - c_n[:, None]).exp().sum(dim=-1).log()).mean()\n",
    "    \n",
    "    return loss / num_chunks\n",
    "\n",
    "loss = manual_implementation_chunked(x, y, A)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse(x, y, A):\n",
    "    loss = 0.\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N)\n",
    "    s_global = torch.zeros(N)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = float(\"-inf\") * torch.ones(chunk_size)\n",
    "        s = torch.zeros(chunk_size)\n",
    "\n",
    "        for v in range(V):\n",
    "            m_prev = m.clone()\n",
    "            z_j = (x_chunk * A[v]).sum(dim=-1)\n",
    "            m = torch.maximum(m_prev, z_j)\n",
    "            s = s * (m_prev - m).exp() + (z_j - m).exp()\n",
    "\n",
    "        loss += (-(A[y_chunk] * x_chunk).sum(dim=-1) + m + s.log()).mean()\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return loss / num_chunks, m_global, s_global\n",
    "\n",
    "loss, m, s = manual_implementation_chunked_online_lse(x, y, A)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd(x, y, A, m_global, s_global):\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    Agrad = torch.zeros_like(A)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H))\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        m = m_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "\n",
    "        xgrad = -A[y_chunk]\n",
    "        for line, y in enumerate(y_chunk):\n",
    "            Agrad[y] += -x_chunk[line]\n",
    "        for v in range(V):\n",
    "            z_j = (x_chunk * A[v]).sum(dim=-1)\n",
    "            xgrad += ((z_j - m).exp() / s)[:, None]  * A[v][None, :]\n",
    "            Agrad[v] += (((z_j - m).exp() / s)[:, None] * x_chunk).sum(dim=0)\n",
    "\n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return Agrad / N, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd(x, y, A, m, s)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_implementation_vectorized_bwd(x, y, A):\n",
    "    Agrad = torch.zeros_like(A)\n",
    "    x_cont = x.view(-1, H)\n",
    "    y_cont = y.view(-1)\n",
    "    # loss = (-(A[y_cont] * x_cont).sum(dim=-1) + z_nv.logsumexp(dim=-1)).mean()\n",
    "    z_nv = x_cont @ A.T\n",
    "    c_n = z_nv.max(dim=-1)[0]\n",
    "    loss = (-(A[y_cont] * x_cont).sum(dim=-1) + c_n + (z_nv - c_n[:, None]).exp().sum(dim=-1).log()).mean() \n",
    "    # bwd \n",
    "    d = (z_nv - c_n[:, None]).exp().sum(dim=-1, keepdim=True)\n",
    "    xgrad = (-A[y_cont] + ((z_nv - c_n[:, None]).exp() / d) @ A) / N\n",
    "    # Agrad[y_cont] += -x_cont\n",
    "    for idx, y in enumerate(y_cont):\n",
    "        Agrad[y] += -x_cont[idx]\n",
    "    Agrad +=((z_nv - c_n[:, None]).exp() / d).T @ x_cont\n",
    "    \n",
    "    return loss, Agrad / N, xgrad.view_as(x)\n",
    "\n",
    "# x.grad.zero_()\n",
    "# A.grad.zero_()\n",
    "loss, Agrad, xgrad = manual_implementation_vectorized_bwd(x, y, A)\n",
    "# loss.backward()\n",
    "# # loss, torch.dist(x.grad, xgrad), \n",
    "loss.item(), torch.dist(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop(x, y, A, m_global, s_global):\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    Agrad = torch.zeros_like(A)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H))\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        m = m_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        xgrad = torch.zeros_like(x_chunk)\n",
    "\n",
    "        for v in range(V):\n",
    "            z_j = (x_chunk * A[v]).sum(dim=-1)\n",
    "            xgrad += ((z_j - m).exp() / s)[:, None]  * A[v][None, :]\n",
    "            Agrad[v] += (((z_j - m).exp() / s)[:, None] * x_chunk).sum(dim=0)\n",
    "\n",
    "            mask = y_chunk == v\n",
    "            if mask.sum() > 0:\n",
    "                Agrad[v] -= x_chunk[mask].sum(dim=0)\n",
    "                xgrad[mask] -= A[v]\n",
    "\n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return Agrad / N, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop(x, y, A, m, s)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_single_loop(x, y, A):\n",
    "    loss = 0.\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N)\n",
    "    s_global = torch.zeros(N)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = float(\"-inf\") * torch.ones(chunk_size)\n",
    "        s = torch.zeros(chunk_size)\n",
    "\n",
    "        for v in range(V):\n",
    "            m_prev = m.clone()\n",
    "            z_j = (x_chunk * A[v]).sum(dim=-1)\n",
    "            m = torch.maximum(m_prev, z_j)\n",
    "            s = s * (m_prev - m).exp() + (z_j - m).exp()\n",
    "            mask = y_chunk == v\n",
    "            if mask.sum() > 0:\n",
    "                loss -= z_j[mask].sum() / N\n",
    "\n",
    "        loss += (m + s.log()).mean() / num_chunks\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return loss, m_global, s_global\n",
    "\n",
    "loss, m, s = manual_implementation_chunked_online_lse_single_loop(x, y, A)\n",
    "loss.item(), torch.dist(loss, reference_loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_single_loop_bf16(x, y, A):\n",
    "    x = x.to(dtype=torch.float16)\n",
    "    A = A.to(dtype=torch.float16)\n",
    "\n",
    "    loss = 0.\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N, dtype=torch.float32)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "        s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "\n",
    "        for v in range(V):\n",
    "            m_prev = m.clone()\n",
    "            z_j = (x_chunk * A[v]).to(dtype=torch.float32).sum(dim=-1)\n",
    "            m = torch.maximum(m_prev, z_j)\n",
    "            s = s * (m_prev - m).exp() + (z_j - m).exp()\n",
    "            mask = y_chunk == v\n",
    "            if mask.sum() > 0:\n",
    "                loss -= z_j[mask].sum() / N\n",
    "\n",
    "        loss += (m + s.log()).mean() / num_chunks\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return loss, m_global, s_global\n",
    "\n",
    "loss, m, s = manual_implementation_chunked_online_lse_single_loop_bf16(x, y, A)\n",
    "loss.item(), torch.dist(loss, reference_loss).item(), f\"{torch.dist(loss, reference_loss).item():2.4e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop_bf16(x, y, A, m_global, s_global):\n",
    "    x = x.to(dtype=torch.float16) # float16 is much more accurate for \"sane\" values of x and A\n",
    "    A = A.to(dtype=torch.float16)\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    Agrad = torch.zeros_like(A, dtype=torch.float32)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        m = m_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        xgrad = torch.zeros_like(x_chunk)\n",
    "\n",
    "        for v in range(V):\n",
    "            z_j = (x_chunk * A[v]).to(dtype=torch.float32).sum(dim=-1)\n",
    "            xgrad += ((z_j - m).exp() / s)[:, None]  * A[v][None, :].to(dtype=torch.float32)\n",
    "            Agrad[v] += (((z_j - m).exp() / s)[:, None] * x_chunk).sum(dim=0)\n",
    "\n",
    "            mask = y_chunk == v\n",
    "            if mask.sum() > 0:\n",
    "                Agrad[v] -= x_chunk[mask].to(dtype=torch.float32).sum(dim=0)\n",
    "                xgrad[mask] -= A[v].to(dtype=torch.float32)\n",
    "\n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return Agrad / N, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop_bf16(x, y, A, m, s)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad\n",
    "# def manual_implementation_chunked_online_lse_single_loop_fwd_bwd_bf16(x, y, A):\n",
    "#     x = x.to(dtype=torch.float16)\n",
    "#     A = A.to(dtype=torch.float16)\n",
    "\n",
    "#     loss = 0.\n",
    "#     x_chunks = x.view(-1, H).split(chunk_size)\n",
    "#     y_chunks = y.view(-1).split(chunk_size)\n",
    "#     num_chunks = len(y_chunks)\n",
    "\n",
    "#     Agrad = torch.zeros_like(A, dtype=torch.float32)\n",
    "#     global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "#     for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "#         m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "#         s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "#         xgrad = torch.zeros_like(x_chunk)\n",
    "#         xgrad2 = torch.zeros_like(x_chunk)\n",
    "\n",
    "#         for v in range(V):\n",
    "#             m_prev = m.clone()\n",
    "#             z_j = (x_chunk * A[v]).to(dtype=torch.float32).sum(dim=-1)\n",
    "#             m = torch.maximum(m_prev, z_j)\n",
    "#             s = s * (m_prev - m).exp() + (z_j - m).exp()\n",
    "#             mask = y_chunk == v\n",
    "#             if mask.sum() > 0:\n",
    "#                 loss -= z_j[mask].sum() / N\n",
    "#                 # bwd 1\n",
    "#                 Agrad[v] -= x_chunk[mask].to(dtype=torch.float32).sum(dim=0)\n",
    "#                 xgrad[mask] -= A[v].to(dtype=torch.float32)\n",
    "\n",
    "#             # bwd 2\n",
    "#             xgrad2 += ((z_j - m).exp())[:, None]  * A[v][None, :].to(dtype=torch.float32) # I think the -m_j is the problem here\n",
    "#             Agrad[v] += (((z_j - m).exp())[:, None] * x_chunk).sum(dim=0)  # m and s are online in this version and still changing\n",
    "\n",
    "#         global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N + xgrad2 / s / N\n",
    "#         Agrad /= s\n",
    "#         loss += (m + s.log()).mean() / num_chunks\n",
    "    \n",
    "#     return loss, Agrad, global_x_grad.view_as(x)\n",
    "\n",
    "# loss, Agrad, xgrad = manual_implementation_chunked_online_lse_single_loop_fwd_bwd_bf16(x, y, A)\n",
    "# loss.item(), f\"{torch.dist(loss, reference_loss).item():2.4e}\", torch.dist(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_chunk_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_chunk_size = 16\n",
    "\n",
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_single_loop_bf16_blockV(x, y, A):\n",
    "    x = x.to(dtype=torch.float16)\n",
    "    A = A.to(dtype=torch.float16)\n",
    "\n",
    "    loss = 0.\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N, dtype=torch.float32)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "        s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks):\n",
    "            m_prev = m.clone()\n",
    "            z_j_to_k = (x_chunk @ A_v.T).to(dtype=torch.float32) \n",
    "            m = torch.maximum(m_prev, z_j_to_k.max(dim=-1)[0])\n",
    "            s = s * (m_prev - m).exp() + (z_j_to_k - m[:, None]).exp().sum(dim=-1)\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "            mask = y_chunk[:, None] == v_range[None, :]\n",
    "            if mask.sum() > 0:\n",
    "                loss -= z_j_to_k[mask].sum() / N\n",
    "\n",
    "        loss += (m + s.log()).mean() / num_chunks\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return loss, m_global, s_global\n",
    "\n",
    "loss, m, s = manual_implementation_chunked_online_lse_single_loop_bf16_blockV(x, y, A)\n",
    "loss.item(), torch.dist(loss, reference_loss).item()# , f\"{torch.dist(loss, reference_loss).item():2.4e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV(x, y, A, m_global, s_global):\n",
    "    x = x.to(dtype=torch.float16) # float16 is much more accurate for \"sane\" values of x and A\n",
    "    A = A.to(dtype=torch.float16)\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    Agrad = torch.zeros_like(A, dtype=torch.float32)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        m = m_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        xgrad = torch.zeros_like(x_chunk)\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks):\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "\n",
    "            z_j_to_k = (x_chunk @ A_v.T).to(dtype=torch.float32) \n",
    "            xgrad += (((z_j_to_k - m[:, None]).exp() / s[:, None]).to(dtype=torch.float16) @  A_v).to(dtype=torch.float32)\n",
    "            Agrad[v_range] += (((z_j_to_k - m[:, None]).exp() / s[:, None]).T.to(dtype=torch.float16) @ x_chunk).to(dtype=torch.float32)\n",
    "\n",
    "            mask = y_chunk[:, None,] == v_range[None, :]\n",
    "            if mask.sum() > 0:\n",
    "                Agrad[v_range] -= torch.where(mask[:,:,None], x_chunk[:, None, :], 0.0).sum(dim=0) # reduction over N\n",
    "                xgrad -= torch.where(mask[:, :, None], A_v, 0.0).sum(dim=1)\n",
    "\n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return Agrad / N, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV(x, y, A, m, s)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_chunk_size = 16\n",
    "\n",
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_single_loop_bf16_blockV(x, y, A):\n",
    "    x = x.to(dtype=torch.float16)\n",
    "    A = A.to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N, dtype=torch.float32)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32)\n",
    "    losses = torch.zeros(num_chunks, dtype=torch.float32)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "        s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks):\n",
    "            m_prev = m.clone()\n",
    "            z_j_to_k = (x_chunk @ A_v.T).to(dtype=torch.float32) \n",
    "            m = torch.maximum(m_prev, z_j_to_k.max(dim=-1)[0])\n",
    "            s = s * (m_prev - m).exp() + (z_j_to_k - m[:, None]).exp().sum(dim=-1)\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "            mask = y_chunk[:, None] == v_range[None, :]\n",
    "            if mask.sum() > 0:\n",
    "                losses[idx] -= z_j_to_k[mask].sum() / N\n",
    "\n",
    "        losses[idx] += (m + s.log()).mean() / num_chunks\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return losses.sum(), m_global, s_global\n",
    "\n",
    "loss, m, s = manual_implementation_chunked_online_lse_single_loop_bf16_blockV(x, y, A)\n",
    "loss.item(), torch.dist(loss, reference_loss).item() # , f\"{torch.dist(loss, reference_loss).item():2.4e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_single_loop_bf16_blockV_exp2(x, y, A):\n",
    "    x = x.to(dtype=torch.float32)\n",
    "    A = A.to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N, dtype=torch.float32)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32)\n",
    "    losses = torch.zeros(num_chunks, dtype=torch.float32)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    log2_const = 1.44269504\n",
    "    \n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "        s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "        v_offsets = torch.arange(0, V_chunk_size)\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks):\n",
    "            m_prev = m.clone()\n",
    "            z_j_to_k = (x_chunk @ A_v.T).to(dtype=torch.float32)\n",
    "            m = torch.maximum(m_prev, z_j_to_k.max(dim=-1)[0].mul(log2_const))\n",
    "            s = s * (m_prev - m).exp2() + (z_j_to_k.mul(log2_const) -  m[:, None]).exp2().sum(dim=-1)\n",
    "\n",
    "            mask = y_chunk[:, None] == v_offsets[None, :]\n",
    "            losses[idx] -= torch.where(mask, z_j_to_k, 0.0).sum() / N\n",
    "\n",
    "            v_offsets += V_chunk_size\n",
    "\n",
    "\n",
    "        losses[idx] += (m + s.log2() ).mean() / num_chunks / log2_const\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return losses.sum(), m_global, s_global\n",
    "\n",
    "loss, m_ref, s_ref = manual_implementation_chunked_online_lse_single_loop_bf16_blockV_exp2(x, y, A)\n",
    "m = m_ref\n",
    "s = s_ref\n",
    "loss.item(), torch.dist(loss, reference_loss).item() # , f\"{torch.dist(loss, reference_loss).item():2.4e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2(x, y, A):\n",
    "    x = x.to(dtype=torch.float32)\n",
    "    A = A.to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "    num_chunks = len(y_chunks)\n",
    "    m_global = torch.zeros(N, dtype=torch.float32)\n",
    "    s_global = torch.zeros(N, dtype=torch.float32)\n",
    "    losses = torch.zeros(num_chunks, dtype=torch.float32)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    \n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "        s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "        v_offsets = torch.arange(0, V_chunk_size)\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks):\n",
    "            m_prev = m.clone()\n",
    "            z_j_to_k = (x_chunk @ A_v.T).to(dtype=torch.float32)\n",
    "            m = torch.maximum(m_prev, z_j_to_k.max(dim=-1)[0])\n",
    "            s = s * (m_prev - m).exp() + (z_j_to_k-  m[:, None]).exp().sum(dim=-1)\n",
    "\n",
    "            mask = y_chunk[:, None] == v_offsets[None, :]\n",
    "            losses[idx] -= torch.where(mask, z_j_to_k, 0.0).sum() / N\n",
    "\n",
    "            v_offsets += V_chunk_size\n",
    "\n",
    "\n",
    "        losses[idx] += (m + s.log()).mean() / num_chunks\n",
    "        m_global[idx * chunk_size: (idx+1)*chunk_size] = m\n",
    "        s_global[idx * chunk_size: (idx+1)*chunk_size] = s\n",
    "    \n",
    "    return losses.sum(), m_global, s_global\n",
    "\n",
    "loss, m_ref, s_ref = manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2(x, y, A)\n",
    "m = m_ref\n",
    "s = s_ref\n",
    "loss.item(), torch.dist(loss, reference_loss).item() # , f\"{torch.dist(loss, reference_loss).item():2.4e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = F.linear(x.double(), A.double()).view(-1, V) # N x V \n",
    "torch.dist(z.logsumexp(dim=1), m + s.log()), torch.dist(s, (z - m[:, None]).exp().sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_chunk_size = 16\n",
    "\n",
    "# @torch.no_grad\n",
    "# def manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2_tileH(x, y, A):\n",
    "#     x = x.to(dtype=torch.float16)\n",
    "#     A = A.to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "#     x_chunks = x.view(-1, H).split(chunk_size)\n",
    "#     y_chunks = y.view(-1).split(chunk_size)\n",
    "#     num_chunks = len(y_chunks)\n",
    "#     m_global = torch.zeros(N,  H // H_chunk_size, dtype=torch.float32)\n",
    "#     s_global = torch.zeros(N,  H // H_chunk_size, dtype=torch.float32)\n",
    "#     losses = torch.zeros(num_chunks, H // H_chunk_size, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "#         for h_idx in range(H // H_chunk_size):\n",
    "#             x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "#             A_chunks = A[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size].split(V_chunk_size)\n",
    "\n",
    "#             m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "#             s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "#             v_offsets = torch.arange(0, V_chunk_size)\n",
    "            \n",
    "#             for v_idx, A_v in enumerate(A_chunks):\n",
    "#                 m_prev = m.clone()\n",
    "#                 z_j_to_k = (x_chunk_h.matmul(A_v.T)).to(dtype=torch.float32)  \n",
    "#                 m = torch.maximum(m_prev, z_j_to_k.max(dim=-1)[0])\n",
    "#                 s = s * (m_prev - m).exp() + (z_j_to_k - m[:, None]).exp().sum(dim=-1)\n",
    "\n",
    "#                 mask = y_chunk[:, None] == v_offsets[None, :]\n",
    "#                 losses[idx] -= torch.where(mask, z_j_to_k, 0.0).sum() / N\n",
    "\n",
    "#                 v_offsets += V_chunk_size\n",
    "\n",
    "\n",
    "#             losses[idx, h_idx] += (m + s.log()).mean() / num_chunks\n",
    "#             m_global[idx * chunk_size: (idx+1)*chunk_size, h_idx] = m\n",
    "#             s_global[idx * chunk_size: (idx+1)*chunk_size, h_idx] = s\n",
    "    \n",
    "#     return losses.sum(), m_global.max(dim=1)[0], s_global.sum(dim=1).log()\n",
    "\n",
    "# loss, m, s = manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2_tileH(x, y, A)\n",
    "# loss.item(), torch.dist(loss, reference_loss).item(), torch.dist(m, m_ref), torch.dist(s, s_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_tile_size = 16\n",
    "\n",
    "# @torch.no_grad\n",
    "# def manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2_tileV(x, y, A):\n",
    "#     x = x.to(dtype=torch.float16)\n",
    "#     A = A.to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "#     x_chunks = x.view(-1, H).split(chunk_size)\n",
    "#     y_chunks = y.view(-1).split(chunk_size)\n",
    "#     num_chunks = len(y_chunks)\n",
    "\n",
    "#     num_V_tiles = V // V_chunk_size // V_tile_size\n",
    "#     m_global = torch.zeros(N,  num_V_tiles, dtype=torch.float32)\n",
    "#     s_global = torch.zeros(N,  num_V_tiles, dtype=torch.float32)\n",
    "#     losses = torch.zeros(num_chunks, num_V_tiles, dtype=torch.float32)\n",
    "\n",
    "#     A_chunks = A.split(V_chunk_size)\n",
    "    \n",
    "\n",
    "#     for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "#         for h_idx in range(H // H_chunk_size):\n",
    "#             for v_tile_idx in range(num_V_tiles):\n",
    "\n",
    "#                 m = -10e5 * torch.ones(chunk_size, dtype=torch.float32)\n",
    "#                 s = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "#                 v_offsets = torch.arange(0, V_chunk_size)\n",
    "\n",
    "#                 A_chunks_in_tile = A_chunks[v_tile_idx * V_tile_size: (v_tile_idx+1) * V_tile_size]\n",
    "\n",
    "#                 for v_idx, A_v in enumerate(A_chunks_in_tile):\n",
    "#                     m_prev = m.clone()\n",
    "#                     z_j_to_k = (x_chunk.matmul(A_v.T)).to(dtype=torch.float32)  \n",
    "#                     m = torch.maximum(m_prev, z_j_to_k.max(dim=-1)[0])\n",
    "#                     s = s * (m_prev - m).exp() + (z_j_to_k - m[:, None]).exp().sum(dim=-1)\n",
    "\n",
    "#                     mask = y_chunk[:, None] == v_offsets[None, :]\n",
    "#                     losses[idx, v_tile_idx] -= torch.where(mask, z_j_to_k, 0.0).sum() / N\n",
    "\n",
    "#                     v_offsets += V_chunk_size\n",
    "\n",
    "\n",
    "#                 # losses[idx, v_tile_idx] += (m + s.log()).mean() / num_chunks\n",
    "#                 m_global[idx * chunk_size: (idx+1)*chunk_size, v_tile_idx] = m\n",
    "#                 s_global[idx * chunk_size: (idx+1)*chunk_size, v_tile_idx] = s\n",
    "    \n",
    "#     # Collect \n",
    "#     m_global_reduced = m_global.max(dim=1)[0]\n",
    "#     # s_global_reduced = s_global * (m_global - m_global_reduced).exp().sum(dim=-1) + (s_global.log() - m_global_reduced).exp().sum(dim=-1)\n",
    "#     # s_global_reduced = (m_global + s_global).sum(dim=1).log()\n",
    "#     s_global_reduced = s_global.log().logsumexp(dim=1)\n",
    "\n",
    "#     # final_loss = (losses.sum() + (m_global + s_global).sum(dim=1).log().mean() / num_chunks)\n",
    "#     final_loss = (losses.sum() + (m_global_reduced + s_global_reduced.log())).mean() / num_chunks\n",
    "\n",
    "    \n",
    "#     return final_loss, m_global_reduced, s_global_reduced\n",
    "\n",
    "# loss, m, s = manual_implementation_chunked_online_lse_single_loop_bf16_blockV_2_tileV(x, y, A)\n",
    "# loss.item(), torch.dist(loss, reference_loss).item(), torch.dist(m, m_ref), torch.dist(s, s_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim(s, s_ref), s, s_ref, s.mean(), s_ref.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_chunk_size = 16\n",
    "V_chunk_size = 64\n",
    "\n",
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH(x, y, A, m_global, s_global):\n",
    "    compute_dtype = torch.float32\n",
    "    x = x.to(dtype=compute_dtype) # float16 is much more accurate for \"sane\" values of x and A\n",
    "    A = A.to(dtype=compute_dtype)\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    Agrad = torch.zeros_like(A, dtype=torch.float32)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        m = m_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        xgrad = torch.zeros_like(x_chunk)\n",
    "        Nc = x_chunk.shape[0]\n",
    "        Vc = A_chunks[0].shape[0]\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks): # can parallelize\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "\n",
    "            z_j_to_k = torch.zeros(Nc, Vc)\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "\n",
    "                z_j_to_k += (x_chunk_h @ A_chunk_h.T).to(dtype=torch.float32)\n",
    "            \n",
    "            softmax_z = ((z_j_to_k - m[:, None]).exp() / s[:, None]).to(dtype=compute_dtype)\n",
    "            mask = y_chunk[:, None,] == v_range[None, :]\n",
    "\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "\n",
    "\n",
    "                xgrad_temp = (softmax_z @  A_chunk_h).to(dtype=torch.float32)\n",
    "                xgrad_temp -= torch.where(mask[:, :, None], A_chunk_h[None, :, :], 0.0).sum(dim=1)\n",
    "                xgrad[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size] += xgrad_temp\n",
    "\n",
    "\n",
    "                Agrad_temp = (softmax_z.T @ x_chunk_h).to(dtype=torch.float32)\n",
    "                Agrad_temp -= torch.where(mask[:,:,None], x_chunk_h[:, None, :], 0.0).sum(dim=0)\n",
    "                Agrad[v_range, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size] += Agrad_temp\n",
    "    \n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return Agrad / N, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH(x, y, A, m, s)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV(x, y, A, m_global, s_global):\n",
    "    compute_dtype = torch.float32\n",
    "    x = x.to(dtype=compute_dtype) # float16 is much more accurate for \"sane\" values of x and A\n",
    "    A = A.to(dtype=compute_dtype)\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    A_chunks = A.split(V_chunk_size)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    Agrad = torch.zeros_like(A, dtype=torch.float32)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        m = m_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        xgrad = torch.zeros_like(x_chunk)\n",
    "\n",
    "        for v_idx, A_v in enumerate(A_chunks):\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "\n",
    "            z_j_to_k = (x_chunk @ A_v.T).to(dtype=torch.float32) \n",
    "            xgrad += (((z_j_to_k - m[:, None]).exp() / s[:, None]).to(dtype=compute_dtype) @  A_v).to(dtype=torch.float32)\n",
    "            Agrad[v_range] += (((z_j_to_k - m[:, None]).exp() / s[:, None]).T.to(dtype=compute_dtype) @ x_chunk).to(dtype=torch.float32)\n",
    "\n",
    "            mask = y_chunk[:, None,] == v_range[None, :]\n",
    "            if mask.sum() > 0:\n",
    "                Agrad[v_range] -= torch.where(mask[:,:,None], x_chunk[:, None, :], 0.0).sum(dim=0) # reduction over N\n",
    "                xgrad -= torch.where(mask[:, :, None], A_v, 0.0).sum(dim=1)\n",
    "\n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return Agrad / N, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV(x, y, A, m, s)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_chunk_size = 16\n",
    "V_chunk_size = 32\n",
    "print(chunk_size, V_chunk_size, H_chunk_size)\n",
    "print(N, V, H)\n",
    "\n",
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH_At(x, y, A, m_global, s_global):\n",
    "    compute_dtype = torch.float32\n",
    "    x = x.to(dtype=compute_dtype) # float16 is much more accurate for \"sane\" values of x and A\n",
    "    At = A.T.to(dtype=compute_dtype)\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    At_chunks = At.split(V_chunk_size, dim=1)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    Atgrad = torch.zeros_like(At, dtype=torch.float32)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        s = s_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        m = m_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        xgrad = torch.zeros_like(x_chunk)\n",
    "        Nc = x_chunk.shape[0]\n",
    "        Vc = At_chunks[0].shape[1]\n",
    "\n",
    "        for v_idx, A_v in enumerate(At_chunks): # can parallelize\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "\n",
    "            z_j_to_k = torch.zeros(Nc, Vc)\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "                z_j_to_k += (x_chunk_h @ A_chunk_h).to(dtype=torch.float32)\n",
    "            \n",
    "            softmax_z = ((z_j_to_k - m[:, None]).exp() / s[:, None]).to(dtype=compute_dtype)\n",
    "            mask = (y_chunk[:, None] == v_range[None, :])[:,:,None] # needs to be N_BLOCK_SIZE x V_BLOCK_SIZE x 1 ?\n",
    "\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "\n",
    "                xgrad_temp = (softmax_z @  A_chunk_h.T).to(dtype=torch.float32)\n",
    "                xgrad_temp -= torch.where(mask, A_chunk_h.T[None, :, :], 0.0).sum(dim=1)\n",
    "                xgrad[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size] += xgrad_temp\n",
    "\n",
    "\n",
    "                Agrad_temp = (softmax_z.T @ x_chunk_h).to(dtype=torch.float32)\n",
    "                Agrad_temp -= torch.where(mask, x_chunk_h[:, None, :], 0.0).sum(dim=0)\n",
    "                Atgrad[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, v_range] += Agrad_temp.T\n",
    "    \n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return (Atgrad / N).T, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH_At(x, y, A, m, s)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_chunk_size = 16\n",
    "V_chunk_size = 32\n",
    "print(chunk_size, V_chunk_size, H_chunk_size)\n",
    "print(N, V, H)\n",
    "lse_global = m + s.log()\n",
    "\n",
    "@torch.no_grad\n",
    "def manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH_At_double_recomp(x, y, A, lse_global):\n",
    "    compute_dtype = torch.float32\n",
    "    x = x.to(dtype=compute_dtype) # float16 is much more accurate for \"sane\" values of x and A\n",
    "    At = A.T.to(dtype=compute_dtype)\n",
    "\n",
    "    x_chunks = x.view(-1, H).split(chunk_size)\n",
    "    y_chunks = y.view(-1).split(chunk_size)\n",
    "\n",
    "    At_chunks = At.split(V_chunk_size, dim=1)\n",
    "    v_offsets = torch.arange(V)\n",
    "\n",
    "    Atgrad = torch.zeros_like(At, dtype=torch.float32)\n",
    "    global_x_grad = torch.zeros_like(x.view(-1, H), dtype=torch.float32)\n",
    "\n",
    "\n",
    "    for v_idx, A_v in enumerate(At_chunks): # can parallelize\n",
    "        v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "\n",
    "        for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "            lse = lse_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "            xgrad = torch.zeros_like(x_chunk)\n",
    "            Nc = x_chunk.shape[0]\n",
    "            Vc = At_chunks[0].shape[1]\n",
    "\n",
    "            z_j_to_k = torch.zeros(Nc, Vc)\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "                z_j_to_k += (x_chunk_h @ A_chunk_h).to(dtype=torch.float32)\n",
    "            \n",
    "            lse = lse_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "            softmax_z = ((z_j_to_k - lse[:, None]).exp()).to(dtype=compute_dtype)\n",
    "            mask = (y_chunk[:, None] == v_range[None, :])[:,:,None] # needs to be N_BLOCK_SIZE x V_BLOCK_SIZE x 1 ?\n",
    "\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "                Agrad_temp = (softmax_z.T @ x_chunk_h).to(dtype=torch.float32)\n",
    "                Agrad_temp -= torch.where(mask, x_chunk_h[:, None, :], 0.0).sum(dim=0)\n",
    "                Atgrad[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, v_range] += Agrad_temp.T\n",
    "    \n",
    "\n",
    "    for idx, (x_chunk, y_chunk) in enumerate(zip(x_chunks, y_chunks)):\n",
    "        lse = lse_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "        xgrad = torch.zeros_like(x_chunk)\n",
    "        Nc = x_chunk.shape[0]\n",
    "        Vc = At_chunks[0].shape[1]\n",
    "\n",
    "        for v_idx, A_v in enumerate(At_chunks): # can parallelize\n",
    "            v_range = v_offsets[v_idx * V_chunk_size : (v_idx+1) * V_chunk_size]\n",
    "\n",
    "            z_j_to_k = torch.zeros(Nc, Vc)\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "                z_j_to_k += (x_chunk_h @ A_chunk_h).to(dtype=torch.float32)\n",
    "            \n",
    "            lse = lse_global[idx * chunk_size: (idx+1)*chunk_size]\n",
    "            softmax_z = ((z_j_to_k - lse[:, None]).exp()).to(dtype=compute_dtype)\n",
    "            mask = (y_chunk[:, None] == v_range[None, :])[:,:,None] # needs to be N_BLOCK_SIZE x V_BLOCK_SIZE x 1 ?\n",
    "\n",
    "            for h_idx in range(H // H_chunk_size):\n",
    "                x_chunk_h = x_chunk[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size]\n",
    "                A_chunk_h = A_v[h_idx*H_chunk_size : (h_idx+1) * H_chunk_size, :]\n",
    "\n",
    "\n",
    "                xgrad_temp = (softmax_z @  A_chunk_h.T).to(dtype=torch.float32)\n",
    "                xgrad_temp -= torch.where(mask, A_chunk_h.T[None, :, :], 0.0).sum(dim=1)\n",
    "                xgrad[:, h_idx*H_chunk_size : (h_idx+1) * H_chunk_size] += xgrad_temp\n",
    "\n",
    "    \n",
    "        global_x_grad[idx * chunk_size: (idx+1)*chunk_size] = xgrad / N\n",
    "    \n",
    "    return (Atgrad / N).T, global_x_grad.view_as(x)\n",
    "\n",
    "Agrad, xgrad = manual_implementation_chunked_online_lse_bwd_single_loop_bf16_blockV_blockH_At_double_recomp(x, y, A, lse_global)\n",
    "torch.dist(reference_x_grad, xgrad), cosim(reference_x_grad, xgrad), torch.dist(reference_A_grad, Agrad), cosim(reference_A_grad, Agrad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
